{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "from os import path, makedirs, rename, remove\n",
    "from time import time\n",
    "import argparse\n",
    "import pickle\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import functools\n",
    "import json\n",
    "import pandas as pd\n",
    "import tensorflow_hub as hub\n",
    "import tensorflow as tf\n",
    "import codecs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of words in vocabulary: 822383\n",
      "The model classifies \"The BBC produced spoof on the â€œReal Housewivesâ€ TV programmes, which has a comedic Islamic State twist, has been criticised by Leftists and Muslims who claim the sketch is offensive\" as a member of the class Film.\n",
      "The model classifies \" \" as a member of the class Artist.\n"
     ]
    }
   ],
   "source": [
    "MODEL_DIRECTORY_CLASSIFICATION = 'estimator_text_classify/perceptron_model'\n",
    "DATA_DIRECTORY = 'estimator_text_classify/data'\n",
    "QUERY_FILENAME = 'sample_query.txt'\n",
    "WORD_METADATA_FILENAME = 'word_metadata.tsv'\n",
    "MAX_VOCABULARY_SIZE = 1000000\n",
    "VERBOSITY = 'info'\n",
    "WORDS_FEATURE = 'words'  # Name of the input words feature.\n",
    "LENGTHS_FEATURE = 'lengths'\n",
    "VOCAB_PROCESSOR_FILENAME = 'vocab_processor.pickle'\n",
    "\n",
    "def process_vocabulary(train_sentences, test_sentences,\n",
    "                       reuse=True, vocabulary_processor=None, extend=False, sequence_lengths=False):\n",
    "    \"\"\"Map words to integers, and then map sentences to integer sequences of length flags.max_doc_len, by truncating and\n",
    "       padding as needed. This leads to an integer matrix of data which is what TensorFlow can work with. The processor\n",
    "       is then saved to disk in a file determined by flags.\n",
    "\n",
    "    Args:\n",
    "       reuse: if True load the vocabulary_processor is loaded from disk if the file exists.\n",
    "       vocabulary_processor: if not None, and it was not loaded from disk, the passed vocabulary_processor is used.\n",
    "       extend: if True the vocabulary processor (loaded or passed) is extended.\n",
    "       sequence_lengths: Whether to list the length of each document.\n",
    "    \"\"\"\n",
    "    \n",
    "    vocabulary_processor_path = path.join(MODEL_DIRECTORY_CLASSIFICATION, VOCAB_PROCESSOR_FILENAME)\n",
    "    # If vocabulary_processor gets created/altered save it.\n",
    "    if reuse and path.isfile(vocabulary_processor_path):\n",
    "        vocabulary_processor = tf.contrib.learn.preprocessing.VocabularyProcessor.restore(vocabulary_processor_path)\n",
    "        save_vocab_processor = extend\n",
    "    elif vocabulary_processor is None:\n",
    "        vocabulary_processor = tf.contrib.learn.preprocessing.VocabularyProcessor(MAX_DOCUMENT_LENGTH)\n",
    "        vocabulary_processor.fit(train_sentences)\n",
    "        save_vocab_processor = True\n",
    "    elif extend:\n",
    "        vocabulary_processor.vocabulary_.freeze(False)\n",
    "        vocabulary_processor.fit(train_sentences)\n",
    "        save_vocab_processor = True\n",
    "    else:\n",
    "        save_vocab_processor = False\n",
    "\n",
    "    if train_sentences is not None:\n",
    "        train_bow = np.array(list(vocabulary_processor.transform(train_sentences)))\n",
    "    else:\n",
    "        train_bow = None\n",
    "    if test_sentences is not None:\n",
    "        test_bow = np.array(list(vocabulary_processor.transform(test_sentences)))\n",
    "    else:\n",
    "        test_bow = None\n",
    "    n_words = len(vocabulary_processor.vocabulary_)\n",
    "    print('Number of words in vocabulary: %d' % n_words)\n",
    "\n",
    "    if save_vocab_processor:\n",
    "        if not path.isdir(MODEL_DIRECTORY):\n",
    "            makedirs(MODEL_DIRECTORY)\n",
    "        vocabulary_processor.save(vocabulary_processor_path)\n",
    "\n",
    "    if sequence_lengths:\n",
    "        def calculate_lengths(arr):\n",
    "            return arr.shape[1] - (arr != 0)[:, ::-1].argmax(axis=1)\n",
    "        train_lengths = calculate_lengths(train_bow) if train_bow is not None else None\n",
    "        test_lengths = calculate_lengths(test_bow) if test_bow is not None else None\n",
    "    else:\n",
    "        train_lengths = test_lengths = None\n",
    "\n",
    "    return train_bow, test_bow, train_lengths, test_lengths, vocabulary_processor, n_words\n",
    "    \n",
    "\n",
    "def estimator_spec_for_softmax_classification(logits, labels, mode, params):\n",
    "    \"\"\"Returns EstimatorSpec instance for softmax classification.\"\"\"\n",
    "    predicted_class = tf.argmax(logits, 1)\n",
    "    if mode == tf.estimator.ModeKeys.PREDICT:\n",
    "        return tf.estimator.EstimatorSpec(\n",
    "            mode=mode,\n",
    "            predictions={\n",
    "                'class': predicted_class,\n",
    "                'prob': tf.nn.softmax(logits)\n",
    "            })\n",
    "\n",
    "    loss = tf.losses.sparse_softmax_cross_entropy(labels=labels, logits=logits)\n",
    "    if mode == tf.estimator.ModeKeys.TRAIN:\n",
    "        with tf.name_scope('OptimizeLoss'):\n",
    "            optimizer = tf.train.AdamOptimizer(learning_rate=params.learning_rate)\n",
    "            train_op = optimizer.minimize(loss, global_step=tf.train.get_global_step())\n",
    "        return tf.estimator.EstimatorSpec(mode=mode, loss=loss, train_op=train_op)\n",
    "\n",
    "    # mode == EVAL\n",
    "    eval_metric_ops = {\n",
    "        'accuracy': tf.metrics.accuracy(labels=labels, predictions=predicted_class)\n",
    "    }\n",
    "    return tf.estimator.EstimatorSpec(mode=mode, loss=loss, eval_metric_ops=eval_metric_ops)\n",
    "\n",
    "def bag_of_words_perceptron_model(features, labels, mode, params):\n",
    "    \"\"\"Perceptron architecture\"\"\"\n",
    "    with tf.variable_scope('Perceptron'):\n",
    "        bow_column = tf.feature_column.categorical_column_with_identity(\n",
    "            WORDS_FEATURE, num_buckets=params.n_words)\n",
    "        # Maps sequences of integers < params.n_words\n",
    "        # to params.output_dim dimensional real-valued vectors\n",
    "        # by taking the mean over the word (i.e. integer index) embedding values.\n",
    "        bow_embedding_column = tf.feature_column.embedding_column(\n",
    "            bow_column, dimension=params.output_dim)\n",
    "        logits = tf.feature_column.input_layer(\n",
    "            features,\n",
    "            feature_columns=[bow_embedding_column])\n",
    "\n",
    "    return estimator_spec_for_softmax_classification(logits, labels, mode, params)\n",
    "\n",
    "def input_fn(x, y=None, lengths=None, batch_size=None, num_epochs=None, shuffle=False):\n",
    "    \"\"\"Generic input function to be used as the input_fn arguments for Experiment or directly with Estimators.\"\"\"\n",
    "    if batch_size is None and x is not None:\n",
    "        batch_size = len(x)\n",
    "    x_dict = {WORDS_FEATURE: x}\n",
    "    if lengths is not None:\n",
    "        x_dict['LENGTHS_FEATURE'] = lengths\n",
    "    return tf.estimator.inputs.numpy_input_fn(\n",
    "        x_dict,\n",
    "        y,\n",
    "        batch_size=batch_size,\n",
    "        num_epochs=num_epochs,\n",
    "        shuffle=shuffle)\n",
    "\n",
    "def predict(x_data, x_lengths, model_fn, output_dim):\n",
    "    \"\"\"Performs classification on the given x_data using the model given by model_fn.\"\"\"\n",
    "    hparams = tf.contrib.training.HParams(\n",
    "        n_words=MAX_VOCABULARY_SIZE,\n",
    "        output_dim=output_dim,\n",
    "    )\n",
    "\n",
    "    run_config = tf.contrib.learn.RunConfig()\n",
    "    run_config = run_config.replace(model_dir=MODEL_DIRECTORY_CLASSIFICATION)\n",
    "    predictions = tf.estimator.Estimator(\n",
    "        model_fn=model_fn,\n",
    "        config=run_config,\n",
    "        params=hparams\n",
    "    ).predict(input_fn(x_data, lengths=x_lengths, num_epochs=1))\n",
    "    return [p['class'] for p in predictions]\n",
    "\n",
    "output_dim = 14\n",
    "with open(QUERY_FILENAME, 'r', encoding='utf-8') as txt:\n",
    "    data = txt.read()\n",
    "\n",
    "queries = np.array(data.split('.'))\n",
    "#queries = np.loadtxt(data, delimiter='.')\n",
    "classes_filename = path.join(DATA_DIRECTORY, 'classes.txt')\n",
    "classes = pd.read_csv(classes_filename, header=None, names=['class'])\n",
    "_, x_query, _, query_lengths, _, _ = process_vocabulary(\n",
    "        None, queries, reuse=True, sequence_lengths=False)\n",
    "classifications = predict(x_query, query_lengths, bag_of_words_perceptron_model, output_dim)\n",
    "for i, query in enumerate(queries):\n",
    "    print('The model classifies \"{}\" as a member of the class {}.'.format(\n",
    "           query, classes['class'][classifications[i]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0402 12:12:37.748011 15104 tf_logging.py:126] Input graph does not use tf.data.Dataset or contain a QueueRunner. That means predict yields forever. This is probably a mistake.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "words: The  BBC   spoof the  â€œReal Housewivesâ€ TV   programmes, a     comedic Islamic State twist, Leftists Muslims the   sketch offensive\n",
      "preds: I-NP I-ORG O     I-NP O       I-NP          I-NP I-ORG       I-ORG I-ORG   O       O     I-NP   I-NP     I-ORG   I-ORG I-ORG  O        \n"
     ]
    }
   ],
   "source": [
    "\n",
    "DATADIR = 'sequence_tagging_ner/data'\n",
    "PARAMS = 'sequence_tagging_ner/results/params.json'\n",
    "MODELDIR = 'sequence_tagging_ner/results/model'\n",
    "def model_fn(features, labels, mode, params):\n",
    "    # For serving, features are a bit different\n",
    "    if isinstance(features, dict):\n",
    "        features = features['words'], features['nwords']\n",
    "\n",
    "    # Read vocabs and inputs\n",
    "    dropout = params['dropout']\n",
    "    words, nwords = features\n",
    "    training = (mode == tf.estimator.ModeKeys.TRAIN)\n",
    "    vocab_words = tf.contrib.lookup.index_table_from_file(\n",
    "        params['words'], num_oov_buckets=params['num_oov_buckets'])\n",
    "    with Path(params['tags']).open() as f:\n",
    "        indices = [idx for idx, tag in enumerate(f) if tag.strip() != 'O']\n",
    "        num_tags = len(indices) + 1\n",
    "\n",
    "    # Word Embeddings\n",
    "    word_ids = vocab_words.lookup(words)\n",
    "    glove = np.load(params['glove'])['embeddings']  # np.array\n",
    "    variable = np.vstack([glove, [[0.]*params['dim']]])\n",
    "    variable = tf.Variable(variable, dtype=tf.float32, trainable=False)\n",
    "    embeddings = tf.nn.embedding_lookup(variable, word_ids)\n",
    "    embeddings = tf.layers.dropout(embeddings, rate=dropout, training=training)\n",
    "\n",
    "    # LSTM\n",
    "    t = tf.transpose(embeddings, perm=[1, 0, 2])\n",
    "    lstm_cell_fw = tf.contrib.rnn.LSTMBlockFusedCell(params['lstm_size'])\n",
    "    lstm_cell_bw = tf.contrib.rnn.LSTMBlockFusedCell(params['lstm_size'])\n",
    "    lstm_cell_bw = tf.contrib.rnn.TimeReversedFusedRNN(lstm_cell_bw)\n",
    "    output_fw, _ = lstm_cell_fw(t, dtype=tf.float32, sequence_length=nwords)\n",
    "    output_bw, _ = lstm_cell_bw(t, dtype=tf.float32, sequence_length=nwords)\n",
    "    output = tf.concat([output_fw, output_bw], axis=-1)\n",
    "    output = tf.transpose(output, perm=[1, 0, 2])\n",
    "    output = tf.layers.dropout(output, rate=dropout, training=training)\n",
    "\n",
    "    # CRF\n",
    "    logits = tf.layers.dense(output, num_tags)\n",
    "    crf_params = tf.get_variable(\"crf\", [num_tags, num_tags], dtype=tf.float32)\n",
    "    pred_ids, _ = tf.contrib.crf.crf_decode(logits, crf_params, nwords)\n",
    "\n",
    "    if mode == tf.estimator.ModeKeys.PREDICT:\n",
    "        # Predictions\n",
    "        reverse_vocab_tags = tf.contrib.lookup.index_to_string_table_from_file(\n",
    "            params['tags'])\n",
    "        pred_strings = reverse_vocab_tags.lookup(tf.to_int64(pred_ids))\n",
    "        predictions = {\n",
    "            'pred_ids': pred_ids,\n",
    "            'tags': pred_strings\n",
    "        }\n",
    "        return tf.estimator.EstimatorSpec(mode, predictions=predictions)\n",
    "    else:\n",
    "        # Loss\n",
    "        vocab_tags = tf.contrib.lookup.index_table_from_file(params['tags'])\n",
    "        tags = vocab_tags.lookup(labels)\n",
    "        log_likelihood, _ = tf.contrib.crf.crf_log_likelihood(\n",
    "            logits, tags, nwords, crf_params)\n",
    "        loss = tf.reduce_mean(-log_likelihood)\n",
    "\n",
    "        # Metrics\n",
    "        weights = tf.sequence_mask(nwords)\n",
    "        metrics = {\n",
    "            'acc': tf.metrics.accuracy(tags, pred_ids, weights),\n",
    "            'precision': precision(tags, pred_ids, num_tags, indices, weights),\n",
    "            'recall': recall(tags, pred_ids, num_tags, indices, weights),\n",
    "            'f1': f1(tags, pred_ids, num_tags, indices, weights),\n",
    "        }\n",
    "        for metric_name, op in metrics.items():\n",
    "            tf.summary.scalar(metric_name, op[1])\n",
    "\n",
    "        if mode == tf.estimator.ModeKeys.EVAL:\n",
    "            return tf.estimator.EstimatorSpec(\n",
    "                mode, loss=loss, eval_metric_ops=metrics)\n",
    "\n",
    "        elif mode == tf.estimator.ModeKeys.TRAIN:\n",
    "            train_op = tf.train.AdamOptimizer().minimize(\n",
    "                loss, global_step=tf.train.get_or_create_global_step())\n",
    "            return tf.estimator.EstimatorSpec(\n",
    "                mode, loss=loss, train_op=train_op)\n",
    "        \n",
    "def pretty_print(line, preds):\n",
    "    words = line.strip().split()\n",
    "    noun_words = []\n",
    "    for x in range(0, len(words)):\n",
    "        if preds[x].decode('utf-8') != 'O':\n",
    "            noun_words.append(words[x])\n",
    "            \n",
    "    lengths = [max(len(w), len(p)) for w, p in zip(noun_words, preds)]\n",
    "    padded_words = [w + (l - len(w)) * ' ' for w, l in zip(noun_words, lengths)]\n",
    "    padded_preds = [p.decode() + (l - len(p)) * ' ' for p, l in zip(preds, lengths)]\n",
    "    print('words: {}'.format(' '.join(padded_words)))\n",
    "    print('preds: {}'.format(' '.join(padded_preds)))\n",
    "\n",
    "\n",
    "def predict_input_fn(line):\n",
    "    # Words\n",
    "    words = [w.encode() for w in line.strip().split()]\n",
    "    nwords = len(words)\n",
    "\n",
    "    # Wrapping in Tensors\n",
    "    words = tf.constant([words], dtype=tf.string)\n",
    "    nwords = tf.constant([nwords], dtype=tf.int32)\n",
    "\n",
    "    return (words, nwords), None\n",
    "\n",
    "with Path(PARAMS).open() as f:\n",
    "    params = json.load(f)\n",
    "\n",
    "    params['words'] = str(Path(DATADIR, 'vocab.words.txt'))\n",
    "    params['chars'] = str(Path(DATADIR, 'vocab.chars.txt'))\n",
    "    params['tags'] = str(Path(DATADIR, 'vocab.tags.txt'))\n",
    "    params['glove'] = str(Path(DATADIR, 'glove.npz'))\n",
    "\n",
    "    estimator = tf.estimator.Estimator(model_fn, MODELDIR, params=params)\n",
    "    for i, query in enumerate(queries):\n",
    "        predict_inpf = functools.partial(predict_input_fn, query)\n",
    "        for pred in estimator.predict(predict_inpf):\n",
    "            pretty_print(query, pred['tags'])\n",
    "            break\n",
    "        break\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
