{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from os import path, makedirs, rename, remove\n",
    "from time import time\n",
    "import argparse\n",
    "import pickle\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from sklearn.utils import shuffle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "DATA_DIRECTORY = 'data'\n",
    "MAX_DOCUMENT_LENGTH = 10\n",
    "MAX_VOCABULARY_SIZE = 1000000\n",
    "EMBEDDING_DIM = 25\n",
    "TF_SEED = 4242\n",
    "MODEL_DIRECTORY = 'perceptron_model'\n",
    "NUM_EPOCHS = 2\n",
    "BATCH_SIZE = 32\n",
    "LEARNING_RATE = 0.04\n",
    "NP_SEED = 1234\n",
    "CHECKPOINTS_PER_EPOCH = 5\n",
    "WORD_METADATA_FILENAME = 'word_metadata.tsv'\n",
    "VOCAB_PROCESSOR_FILENAME = 'vocab_processor.pickle'\n",
    "DATA_FILENAME = 'data.pickle'\n",
    "VERBOSITY = 'info'\n",
    "WORDS_FEATURE = 'words'  # Name of the input words feature.\n",
    "LENGTHS_FEATURE = 'lengths'  # Name of the document lengths feature (not used for BOW)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Timing functions (MATLAB style)\n",
    "\"\"\"\n",
    "_tstart_stack = []\n",
    "def tic():\n",
    "    _tstart_stack.append(time())\n",
    "\n",
    "\n",
    "def toc(fmt=\"Elapsed: %.2f s\"):\n",
    "    print(fmt % (time() - _tstart_stack.pop()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_data(data_directory, classes_only=False):\n",
    "    \"\"\"Download the DBpedia data if necessary, and load data from the data_directory. If the files train.csv, test.csv\n",
    "       and classes.txt are all in data_directory, then they are used (no download).\"\"\"\n",
    "    # The function call load_dataset in the TensorFlow API is supposed to provide this functionality. However, there are\n",
    "    # currently issues: https://github.com/tensorflow/tensorflow/issues/14698\n",
    "\n",
    "    train_filename = path.join(data_directory, 'train.csv')\n",
    "    test_filename = path.join(data_directory, 'test.csv')\n",
    "    classes_filename = path.join(data_directory, 'classes.txt')\n",
    "    has_train = path.isfile(train_filename)\n",
    "    has_test = path.isfile(test_filename)\n",
    "    has_classes = path.isfile(classes_filename)\n",
    "\n",
    "    if not has_train or not has_test or not has_classes:\n",
    "        # Download the data if necessary, using the API.\n",
    "        tf.contrib.learn.datasets.text_datasets.maybe_download_dbpedia(data_directory)\n",
    "        csv_subdir = 'dbpedia_csv'\n",
    "\n",
    "        if has_train:\n",
    "            remove(train_filename)\n",
    "        rename(path.join(data_directory, csv_subdir, 'train.csv'), train_filename)\n",
    "        if has_test:\n",
    "            remove(test_filename)\n",
    "        rename(path.join(data_directory, csv_subdir, 'test.csv'), test_filename)\n",
    "        if has_classes:\n",
    "            remove(classes_filename)\n",
    "        rename(path.join(data_directory, csv_subdir, 'classes.txt'), classes_filename)\n",
    "\n",
    "    classes = pd.read_csv(classes_filename, header=None, names=['class'])\n",
    "    if classes_only:\n",
    "        return classes\n",
    "    train_raw = pd.read_csv(train_filename, header=None)\n",
    "    test_raw = pd.read_csv(test_filename, header=None)\n",
    "    longest_sent = max([len(sent) for sent in tf.contrib.learn.preprocessing.tokenizer(train_raw[2])])\n",
    "    print(\"The longest sentence in the training data has {} words.\".format(longest_sent))\n",
    "\n",
    "    return train_raw, test_raw, classes\n",
    "\n",
    "\n",
    "def extract_data(train_raw, test_raw):\n",
    "    \"\"\"Extract the document and class from each entry in the data.\"\"\"\n",
    "    x_train = train_raw[2]\n",
    "    y_train = train_raw[0] - 1  # Start enumeration at 0 instead of 1\n",
    "    x_test = test_raw[2]\n",
    "    y_test = test_raw[0] - 1\n",
    "    print('Size of training set: {0}'.format(len(x_train)))\n",
    "    print('Size of test set: {0}'.format(len(x_test)))\n",
    "    return x_train, np.array(y_train), x_test, np.array(y_test)\n",
    "\n",
    "def process_vocabulary(train_sentences, test_sentences,\n",
    "                       reuse=True, vocabulary_processor=None, extend=False, sequence_lengths=False):\n",
    "    \"\"\"Map words to integers, and then map sentences to integer sequences of length flags.max_doc_len, by truncating and\n",
    "       padding as needed. This leads to an integer matrix of data which is what TensorFlow can work with. The processor\n",
    "       is then saved to disk in a file determined by flags.\n",
    "\n",
    "    Args:\n",
    "       reuse: if True load the vocabulary_processor is loaded from disk if the file exists.\n",
    "       vocabulary_processor: if not None, and it was not loaded from disk, the passed vocabulary_processor is used.\n",
    "       extend: if True the vocabulary processor (loaded or passed) is extended.\n",
    "       sequence_lengths: Whether to list the length of each document.\n",
    "    \"\"\"\n",
    "    \n",
    "    vocabulary_processor_path = path.join(MODEL_DIRECTORY, VOCAB_PROCESSOR_FILENAME)\n",
    "    # If vocabulary_processor gets created/altered save it.\n",
    "    if reuse and path.isfile(vocabulary_processor_path):\n",
    "        vocabulary_processor = tf.contrib.learn.preprocessing.VocabularyProcessor.restore(vocabulary_processor_path)\n",
    "        save_vocab_processor = extend\n",
    "    elif vocabulary_processor is None:\n",
    "        vocabulary_processor = tf.contrib.learn.preprocessing.VocabularyProcessor(MAX_DOCUMENT_LENGTH)\n",
    "        vocabulary_processor.fit(train_sentences)\n",
    "        save_vocab_processor = True\n",
    "    elif extend:\n",
    "        vocabulary_processor.vocabulary_.freeze(False)\n",
    "        vocabulary_processor.fit(train_sentences)\n",
    "        save_vocab_processor = True\n",
    "    else:\n",
    "        save_vocab_processor = False\n",
    "\n",
    "    if train_sentences is not None:\n",
    "        train_bow = np.array(list(vocabulary_processor.transform(train_sentences)))\n",
    "    else:\n",
    "        train_bow = None\n",
    "    if test_sentences is not None:\n",
    "        test_bow = np.array(list(vocabulary_processor.transform(test_sentences)))\n",
    "    else:\n",
    "        test_bow = None\n",
    "    n_words = len(vocabulary_processor.vocabulary_)\n",
    "    print('Number of words in vocabulary: %d' % n_words)\n",
    "\n",
    "    if save_vocab_processor:\n",
    "        if not path.isdir(MODEL_DIRECTORY):\n",
    "            makedirs(MODEL_DIRECTORY)\n",
    "        vocabulary_processor.save(vocabulary_processor_path)\n",
    "\n",
    "    if sequence_lengths:\n",
    "        def calculate_lengths(arr):\n",
    "            return arr.shape[1] - (arr != 0)[:, ::-1].argmax(axis=1)\n",
    "        train_lengths = calculate_lengths(train_bow) if train_bow is not None else None\n",
    "        test_lengths = calculate_lengths(test_bow) if test_bow is not None else None\n",
    "    else:\n",
    "        train_lengths = test_lengths = None\n",
    "\n",
    "    return train_bow, test_bow, train_lengths, test_lengths, vocabulary_processor, n_words\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def preprocess_data(sequence_lengths=False):\n",
    "    '''\n",
    "    Load data, shuffle it, process the vocabulary and save to DATA_FILENAME, if not done already.\n",
    "    Returns processed data. NOTE: If the max_doc_len changes from a previous run,\n",
    "    then DATA_FILENAME should be deleted so that it can be properly recreated.\n",
    "    '''\n",
    "    preprocessed_path = path.join(MODEL_DIRECTORY, DATA_FILENAME)\n",
    "    if path.isfile(preprocessed_path):\n",
    "        with open(preprocessed_path, 'rb') as f:\n",
    "            train_raw, x_train, y_train, x_test, y_test, \\\n",
    "            train_lengths, test_lengths, classes = pickle.load(f)\n",
    "    else:\n",
    "        #Get the raw data, downloading if neccessary \n",
    "        train_raw, test_raw, classes = get_data(DATA_DIRECTORY)\n",
    "        \n",
    "        #Seeding is neccessary for reproducability\n",
    "        np.random.seed(TF_SEED)\n",
    "        \n",
    "        # Shuffle data to make the distribution of classes roughly stratified for each mini-batch.\n",
    "        # This is not necessary for full batch training, but is essential for mini-batch training.\n",
    "        train_raw = shuffle(train_raw)\n",
    "        test_raw = shuffle(test_raw)\n",
    "        train_sentences, y_train, test_sentences, y_test = extract_data(train_raw, test_raw)\n",
    "        # Encode the raw data as integer vectors.\n",
    "        x_train, x_test, train_lengths, test_lengths, _, _ = process_vocabulary(\n",
    "            train_sentences, test_sentences,\n",
    "            reuse=True, sequence_lengths=sequence_lengths)\n",
    "        # Save the processed data to avoid re-processing.\n",
    "        saved = False\n",
    "        with open(preprocessed_path, 'wb') as f:\n",
    "            try:\n",
    "                pickle.dump([train_raw, x_train, y_train, x_test, y_test,\n",
    "                             train_lengths, test_lengths, classes], f)\n",
    "                saved = True\n",
    "            except (OverflowError, MemoryError):\n",
    "                # Can happen if max-doc-len is large.\n",
    "                pass\n",
    "\n",
    "        if not saved:\n",
    "            remove(preprocessed_path)\n",
    "\n",
    "    return train_raw, x_train, y_train, x_test, y_test, train_lengths, test_lengths, classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Modelling: Training, evaluation and prediction. Also metadata for TensorBoard.\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "def input_fn(x, y=None, lengths=None, batch_size=None, num_epochs=None, shuffle=False):\n",
    "    \"\"\"Generic input function to be used as the input_fn arguments for Experiment or directly with Estimators.\"\"\"\n",
    "    if batch_size is None and x is not None:\n",
    "        batch_size = len(x)\n",
    "    x_dict = {WORDS_FEATURE: x}\n",
    "    if lengths is not None:\n",
    "        x_dict['LENGTHS_FEATURE'] = lengths\n",
    "    return tf.estimator.inputs.numpy_input_fn(\n",
    "        x_dict,\n",
    "        y,\n",
    "        batch_size=batch_size,\n",
    "        num_epochs=num_epochs,\n",
    "        shuffle=shuffle)\n",
    "\n",
    "\n",
    "def run_experiment(x_train, y_train, x_dev, y_dev, model_fn, schedule, output_dim, train_lengths=None, dev_lengths=None):\n",
    "    \"\"\"Create experiment object and run it.\"\"\"\n",
    "    hparams = tf.contrib.training.HParams(\n",
    "        n_words=MAX_VOCABULARY_SIZE,\n",
    "        n_epochs=NUM_EPOCHS,\n",
    "        seed=TF_SEED,\n",
    "        batch_size=BATCH_SIZE,\n",
    "        learning_rate=LEARNING_RATE,\n",
    "        output_dim=output_dim)\n",
    "\n",
    "    is_training = schedule in ['train', 'train_and_evaluate']\n",
    "    run_config = tf.contrib.learn.RunConfig()\n",
    "    try:\n",
    "        checkpoint_steps = len(x_train) / CHECKPOINTS_PER_EPOCH / BATCH_SIZE if is_training else None\n",
    "        log_step_count_steps = 100  # default value\n",
    "    except TypeError:\n",
    "        # Happens if batch_size is None\n",
    "        checkpoint_steps = 1\n",
    "        log_step_count_steps = 1\n",
    "    run_config = run_config.replace(model_dir=MODEL_DIRECTORY,\n",
    "                                    save_checkpoints_steps=checkpoint_steps,\n",
    "                                    log_step_count_steps=log_step_count_steps,\n",
    "                                    tf_random_seed=hparams.seed)\n",
    "\n",
    "    def experiment_fn(run_config, hparams):\n",
    "        estimator = tf.estimator.Estimator(\n",
    "            model_fn=model_fn,\n",
    "            config=run_config,\n",
    "            params=hparams)\n",
    "        experiment = tf.contrib.learn.Experiment(\n",
    "            estimator=estimator,\n",
    "            train_input_fn=input_fn(x_train, y_train, train_lengths,\n",
    "                                    batch_size=hparams.batch_size,\n",
    "                                    num_epochs=hparams.n_epochs,\n",
    "                                    shuffle=True),\n",
    "            eval_input_fn=input_fn(x_dev, y_dev, dev_lengths,\n",
    "                                   num_epochs=1),\n",
    "            eval_delay_secs=0)\n",
    "        return experiment\n",
    "\n",
    "    if is_training:\n",
    "        print('Training model for {} epochs...'.format(hparams.n_epochs))\n",
    "    tf.contrib.learn.learn_runner.run(\n",
    "        experiment_fn=experiment_fn,\n",
    "        run_config=run_config,\n",
    "        schedule=schedule,  # What to run, e.g. \"train_and_evaluate\", \"evaluate\", ...\n",
    "        hparams=hparams)  # hyperparameters\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def estimator_spec_for_softmax_classification(logits, labels, mode, params):\n",
    "    \"\"\"Returns EstimatorSpec instance for softmax classification.\"\"\"\n",
    "    predicted_class = tf.argmax(logits, 1)\n",
    "    if mode == tf.estimator.ModeKeys.PREDICT:\n",
    "        return tf.estimator.EstimatorSpec(\n",
    "            mode=mode,\n",
    "            predictions={\n",
    "                'class': predicted_class,\n",
    "                'prob': tf.nn.softmax(logits)\n",
    "            })\n",
    "\n",
    "    loss = tf.losses.sparse_softmax_cross_entropy(labels=labels, logits=logits)\n",
    "    if mode == tf.estimator.ModeKeys.TRAIN:\n",
    "        with tf.name_scope('OptimizeLoss'):\n",
    "            optimizer = tf.train.AdamOptimizer(learning_rate=params.learning_rate)\n",
    "            train_op = optimizer.minimize(loss, global_step=tf.train.get_global_step())\n",
    "        return tf.estimator.EstimatorSpec(mode=mode, loss=loss, train_op=train_op)\n",
    "\n",
    "    # mode == EVAL\n",
    "    eval_metric_ops = {\n",
    "        'accuracy': tf.metrics.accuracy(labels=labels, predictions=predicted_class)\n",
    "    }\n",
    "    return tf.estimator.EstimatorSpec(mode=mode, loss=loss, eval_metric_ops=eval_metric_ops)\n",
    "\n",
    "def bag_of_words_perceptron_model(features, labels, mode, params):\n",
    "    \"\"\"Perceptron architecture\"\"\"\n",
    "    with tf.variable_scope('Perceptron'):\n",
    "        bow_column = tf.feature_column.categorical_column_with_identity(\n",
    "            WORDS_FEATURE, num_buckets=params.n_words)\n",
    "        # Maps sequences of integers < params.n_words\n",
    "        # to params.output_dim dimensional real-valued vectors\n",
    "        # by taking the mean over the word (i.e. integer index) embedding values.\n",
    "        bow_embedding_column = tf.feature_column.embedding_column(\n",
    "            bow_column, dimension=params.output_dim)\n",
    "        logits = tf.feature_column.input_layer(\n",
    "            features,\n",
    "            feature_columns=[bow_embedding_column])\n",
    "\n",
    "    return estimator_spec_for_softmax_classification(logits, labels, mode, params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training model for 2 epochs...\n",
      "WARNING:tensorflow:RunConfig.uid (from tensorflow.contrib.learn.python.learn.estimators.run_config) is experimental and may change or be removed at any time, and without warning.\n",
      "INFO:tensorflow:Using config: {'_environment': 'local', '_keep_checkpoint_every_n_hours': 10000, '_save_checkpoints_steps': 3500.0, '_model_dir': 'perceptron_model', '_num_ps_replicas': 0, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x0000027D02EC3DD8>, '_log_step_count_steps': 100, '_keep_checkpoint_max': 5, '_num_worker_replicas': 0, '_tf_config': gpu_options {\n",
      "  per_process_gpu_memory_fraction: 1\n",
      "}\n",
      ", '_evaluation_master': '', '_task_type': None, '_session_config': None, '_tf_random_seed': 4242, '_task_id': 0, '_save_checkpoints_secs': None, '_is_chief': True, '_save_summary_steps': 100, '_master': ''}\n",
      "WARNING:tensorflow:RunConfig.uid (from tensorflow.contrib.learn.python.learn.estimators.run_config) is experimental and may change or be removed at any time, and without warning.\n",
      "WARNING:tensorflow:From C:\\Users\\I349732\\AppData\\Local\\Continuum\\anaconda3\\envs\\tfdeeplearning\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\monitors.py:269: BaseMonitor.__init__ (from tensorflow.contrib.learn.python.learn.monitors) is deprecated and will be removed after 2016-12-05.\n",
      "Instructions for updating:\n",
      "Monitors are deprecated. Please use tf.train.SessionRunHook.\n",
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Saving checkpoints for 1 into perceptron_model\\model.ckpt.\n",
      "INFO:tensorflow:Starting evaluation at 2019-03-26-14:30:50\n",
      "INFO:tensorflow:Restoring parameters from perceptron_model\\model.ckpt-1\n",
      "INFO:tensorflow:Evaluation [1/100]\n",
      "INFO:tensorflow:Finished evaluation at 2019-03-26-14:30:51\n",
      "INFO:tensorflow:Saving dict for global step 1: accuracy = 0.114171, global_step = 1, loss = 2.62983\n",
      "INFO:tensorflow:Validation (step 1): accuracy = 0.114171, global_step = 1, loss = 2.62983\n",
      "INFO:tensorflow:step = 1, loss = 2.60109\n",
      "INFO:tensorflow:global_step/sec: 7.23154\n",
      "INFO:tensorflow:step = 101, loss = 1.92498 (12.173 sec)\n",
      "INFO:tensorflow:global_step/sec: 8.20997\n",
      "INFO:tensorflow:step = 201, loss = 1.43868 (12.180 sec)\n",
      "INFO:tensorflow:global_step/sec: 8.20794\n",
      "INFO:tensorflow:step = 301, loss = 1.00828 (12.183 sec)\n",
      "INFO:tensorflow:global_step/sec: 8.20119\n",
      "INFO:tensorflow:step = 401, loss = 0.873912 (12.193 sec)\n",
      "INFO:tensorflow:global_step/sec: 8.13696\n",
      "INFO:tensorflow:step = 501, loss = 0.993236 (12.289 sec)\n",
      "INFO:tensorflow:global_step/sec: 8.1636\n",
      "INFO:tensorflow:step = 601, loss = 0.523701 (12.251 sec)\n",
      "INFO:tensorflow:global_step/sec: 8.17698\n",
      "INFO:tensorflow:step = 701, loss = 0.651163 (12.229 sec)\n",
      "INFO:tensorflow:global_step/sec: 8.24049\n",
      "INFO:tensorflow:step = 801, loss = 0.73718 (12.135 sec)\n",
      "INFO:tensorflow:global_step/sec: 8.16761\n",
      "INFO:tensorflow:step = 901, loss = 0.659755 (12.243 sec)\n",
      "INFO:tensorflow:global_step/sec: 8.12436\n",
      "INFO:tensorflow:step = 1001, loss = 0.862489 (12.318 sec)\n",
      "INFO:tensorflow:global_step/sec: 7.89603\n",
      "INFO:tensorflow:step = 1101, loss = 0.644803 (12.656 sec)\n",
      "INFO:tensorflow:global_step/sec: 8.14959\n",
      "INFO:tensorflow:step = 1201, loss = 0.374861 (12.271 sec)\n",
      "INFO:tensorflow:global_step/sec: 8.18235\n",
      "INFO:tensorflow:step = 1301, loss = 0.446653 (12.222 sec)\n",
      "INFO:tensorflow:global_step/sec: 8.10917\n",
      "INFO:tensorflow:step = 1401, loss = 0.39006 (12.331 sec)\n",
      "INFO:tensorflow:global_step/sec: 8.1237\n",
      "INFO:tensorflow:step = 1501, loss = 0.434494 (12.310 sec)\n",
      "INFO:tensorflow:global_step/sec: 8.19378\n",
      "INFO:tensorflow:step = 1601, loss = 0.365618 (12.203 sec)\n",
      "INFO:tensorflow:global_step/sec: 8.16962\n",
      "INFO:tensorflow:step = 1701, loss = 0.706742 (12.241 sec)\n",
      "INFO:tensorflow:global_step/sec: 8.11313\n",
      "INFO:tensorflow:step = 1801, loss = 0.344442 (12.325 sec)\n",
      "INFO:tensorflow:global_step/sec: 8.20187\n",
      "INFO:tensorflow:step = 1901, loss = 0.491046 (12.193 sec)\n",
      "INFO:tensorflow:global_step/sec: 8.13563\n",
      "INFO:tensorflow:step = 2001, loss = 0.396791 (12.300 sec)\n",
      "INFO:tensorflow:global_step/sec: 8.17163\n",
      "INFO:tensorflow:step = 2101, loss = 0.380022 (12.228 sec)\n",
      "INFO:tensorflow:global_step/sec: 7.95206\n",
      "INFO:tensorflow:step = 2201, loss = 0.274792 (12.577 sec)\n",
      "INFO:tensorflow:global_step/sec: 7.88292\n",
      "INFO:tensorflow:step = 2301, loss = 0.651056 (12.686 sec)\n",
      "INFO:tensorflow:global_step/sec: 8.17954\n",
      "INFO:tensorflow:step = 2401, loss = 0.154586 (12.224 sec)\n",
      "INFO:tensorflow:global_step/sec: 8.11313\n",
      "INFO:tensorflow:step = 2501, loss = 0.432302 (12.326 sec)\n",
      "INFO:tensorflow:global_step/sec: 7.30345\n",
      "INFO:tensorflow:step = 2601, loss = 0.247211 (13.692 sec)\n",
      "INFO:tensorflow:global_step/sec: 8.11973\n",
      "INFO:tensorflow:step = 2701, loss = 0.191509 (12.317 sec)\n",
      "INFO:tensorflow:global_step/sec: 8.24594\n",
      "INFO:tensorflow:step = 2801, loss = 0.149297 (12.127 sec)\n",
      "INFO:tensorflow:global_step/sec: 7.91034\n",
      "INFO:tensorflow:step = 2901, loss = 0.329502 (12.642 sec)\n",
      "INFO:tensorflow:global_step/sec: 7.49371\n",
      "INFO:tensorflow:step = 3001, loss = 0.381705 (13.347 sec)\n",
      "INFO:tensorflow:global_step/sec: 7.87348\n",
      "INFO:tensorflow:step = 3101, loss = 0.208427 (12.699 sec)\n",
      "INFO:tensorflow:global_step/sec: 8.37533\n",
      "INFO:tensorflow:step = 3201, loss = 0.265721 (11.940 sec)\n",
      "INFO:tensorflow:global_step/sec: 8.41916\n",
      "INFO:tensorflow:step = 3301, loss = 0.521938 (11.878 sec)\n",
      "INFO:tensorflow:global_step/sec: 8.48362\n",
      "INFO:tensorflow:step = 3401, loss = 0.306711 (11.787 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 3501 into perceptron_model\\model.ckpt.\n"
     ]
    },
    {
     "ename": "UnknownError",
     "evalue": "Failed to create a NewWriteableFile: perceptron_model\\model.ckpt-3501.index : Access is denied.\r\n; Input/output error\n\t [[Node: save/MergeV2Checkpoints = MergeV2Checkpoints[delete_old_dirs=true, _device=\"/job:localhost/replica:0/task:0/cpu:0\"](save/MergeV2Checkpoints/checkpoint_prefixes, _arg_save/Const_0_0)]]\n\nCaused by op 'save/MergeV2Checkpoints', defined at:\n  File \"C:\\Users\\I349732\\AppData\\Local\\Continuum\\anaconda3\\envs\\tfdeeplearning\\lib\\runpy.py\", line 193, in _run_module_as_main\n    \"__main__\", mod_spec)\n  File \"C:\\Users\\I349732\\AppData\\Local\\Continuum\\anaconda3\\envs\\tfdeeplearning\\lib\\runpy.py\", line 85, in _run_code\n    exec(code, run_globals)\n  File \"C:\\Users\\I349732\\AppData\\Local\\Continuum\\anaconda3\\envs\\tfdeeplearning\\lib\\site-packages\\ipykernel_launcher.py\", line 16, in <module>\n    app.launch_new_instance()\n  File \"C:\\Users\\I349732\\AppData\\Local\\Continuum\\anaconda3\\envs\\tfdeeplearning\\lib\\site-packages\\traitlets\\config\\application.py\", line 658, in launch_instance\n    app.start()\n  File \"C:\\Users\\I349732\\AppData\\Local\\Continuum\\anaconda3\\envs\\tfdeeplearning\\lib\\site-packages\\ipykernel\\kernelapp.py\", line 477, in start\n    ioloop.IOLoop.instance().start()\n  File \"C:\\Users\\I349732\\AppData\\Local\\Continuum\\anaconda3\\envs\\tfdeeplearning\\lib\\site-packages\\zmq\\eventloop\\ioloop.py\", line 177, in start\n    super(ZMQIOLoop, self).start()\n  File \"C:\\Users\\I349732\\AppData\\Local\\Continuum\\anaconda3\\envs\\tfdeeplearning\\lib\\site-packages\\tornado\\ioloop.py\", line 888, in start\n    handler_func(fd_obj, events)\n  File \"C:\\Users\\I349732\\AppData\\Local\\Continuum\\anaconda3\\envs\\tfdeeplearning\\lib\\site-packages\\tornado\\stack_context.py\", line 277, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"C:\\Users\\I349732\\AppData\\Local\\Continuum\\anaconda3\\envs\\tfdeeplearning\\lib\\site-packages\\zmq\\eventloop\\zmqstream.py\", line 440, in _handle_events\n    self._handle_recv()\n  File \"C:\\Users\\I349732\\AppData\\Local\\Continuum\\anaconda3\\envs\\tfdeeplearning\\lib\\site-packages\\zmq\\eventloop\\zmqstream.py\", line 472, in _handle_recv\n    self._run_callback(callback, msg)\n  File \"C:\\Users\\I349732\\AppData\\Local\\Continuum\\anaconda3\\envs\\tfdeeplearning\\lib\\site-packages\\zmq\\eventloop\\zmqstream.py\", line 414, in _run_callback\n    callback(*args, **kwargs)\n  File \"C:\\Users\\I349732\\AppData\\Local\\Continuum\\anaconda3\\envs\\tfdeeplearning\\lib\\site-packages\\tornado\\stack_context.py\", line 277, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"C:\\Users\\I349732\\AppData\\Local\\Continuum\\anaconda3\\envs\\tfdeeplearning\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 283, in dispatcher\n    return self.dispatch_shell(stream, msg)\n  File \"C:\\Users\\I349732\\AppData\\Local\\Continuum\\anaconda3\\envs\\tfdeeplearning\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 235, in dispatch_shell\n    handler(stream, idents, msg)\n  File \"C:\\Users\\I349732\\AppData\\Local\\Continuum\\anaconda3\\envs\\tfdeeplearning\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 399, in execute_request\n    user_expressions, allow_stdin)\n  File \"C:\\Users\\I349732\\AppData\\Local\\Continuum\\anaconda3\\envs\\tfdeeplearning\\lib\\site-packages\\ipykernel\\ipkernel.py\", line 196, in do_execute\n    res = shell.run_cell(code, store_history=store_history, silent=silent)\n  File \"C:\\Users\\I349732\\AppData\\Local\\Continuum\\anaconda3\\envs\\tfdeeplearning\\lib\\site-packages\\ipykernel\\zmqshell.py\", line 533, in run_cell\n    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n  File \"C:\\Users\\I349732\\AppData\\Local\\Continuum\\anaconda3\\envs\\tfdeeplearning\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2698, in run_cell\n    interactivity=interactivity, compiler=compiler, result=result)\n  File \"C:\\Users\\I349732\\AppData\\Local\\Continuum\\anaconda3\\envs\\tfdeeplearning\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2802, in run_ast_nodes\n    if self.run_code(code, result):\n  File \"C:\\Users\\I349732\\AppData\\Local\\Continuum\\anaconda3\\envs\\tfdeeplearning\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2862, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"<ipython-input-21-6781a4ae384f>\", line 5, in <module>\n    bag_of_words_perceptron_model, 'train_and_evaluate', output_dim)\n  File \"<ipython-input-18-e856e4f3be98>\", line 67, in run_experiment\n    hparams=hparams)  # hyperparameters\n  File \"C:\\Users\\I349732\\AppData\\Local\\Continuum\\anaconda3\\envs\\tfdeeplearning\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\learn_runner.py\", line 209, in run\n    return _execute_schedule(experiment, schedule)\n  File \"C:\\Users\\I349732\\AppData\\Local\\Continuum\\anaconda3\\envs\\tfdeeplearning\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\learn_runner.py\", line 46, in _execute_schedule\n    return task()\n  File \"C:\\Users\\I349732\\AppData\\Local\\Continuum\\anaconda3\\envs\\tfdeeplearning\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\experiment.py\", line 502, in train_and_evaluate\n    self.train(delay_secs=0)\n  File \"C:\\Users\\I349732\\AppData\\Local\\Continuum\\anaconda3\\envs\\tfdeeplearning\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\experiment.py\", line 280, in train\n    hooks=self._train_monitors + extra_hooks)\n  File \"C:\\Users\\I349732\\AppData\\Local\\Continuum\\anaconda3\\envs\\tfdeeplearning\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\experiment.py\", line 672, in _call_train\n    hooks=hooks)\n  File \"C:\\Users\\I349732\\AppData\\Local\\Continuum\\anaconda3\\envs\\tfdeeplearning\\lib\\site-packages\\tensorflow\\python\\estimator\\estimator.py\", line 241, in train\n    loss = self._train_model(input_fn=input_fn, hooks=hooks)\n  File \"C:\\Users\\I349732\\AppData\\Local\\Continuum\\anaconda3\\envs\\tfdeeplearning\\lib\\site-packages\\tensorflow\\python\\estimator\\estimator.py\", line 683, in _train_model\n    log_step_count_steps=self._config.log_step_count_steps) as mon_sess:\n  File \"C:\\Users\\I349732\\AppData\\Local\\Continuum\\anaconda3\\envs\\tfdeeplearning\\lib\\site-packages\\tensorflow\\python\\training\\monitored_session.py\", line 365, in MonitoredTrainingSession\n    stop_grace_period_secs=stop_grace_period_secs)\n  File \"C:\\Users\\I349732\\AppData\\Local\\Continuum\\anaconda3\\envs\\tfdeeplearning\\lib\\site-packages\\tensorflow\\python\\training\\monitored_session.py\", line 668, in __init__\n    stop_grace_period_secs=stop_grace_period_secs)\n  File \"C:\\Users\\I349732\\AppData\\Local\\Continuum\\anaconda3\\envs\\tfdeeplearning\\lib\\site-packages\\tensorflow\\python\\training\\monitored_session.py\", line 490, in __init__\n    self._sess = _RecoverableSession(self._coordinated_creator)\n  File \"C:\\Users\\I349732\\AppData\\Local\\Continuum\\anaconda3\\envs\\tfdeeplearning\\lib\\site-packages\\tensorflow\\python\\training\\monitored_session.py\", line 842, in __init__\n    _WrappedSession.__init__(self, self._create_session())\n  File \"C:\\Users\\I349732\\AppData\\Local\\Continuum\\anaconda3\\envs\\tfdeeplearning\\lib\\site-packages\\tensorflow\\python\\training\\monitored_session.py\", line 847, in _create_session\n    return self._sess_creator.create_session()\n  File \"C:\\Users\\I349732\\AppData\\Local\\Continuum\\anaconda3\\envs\\tfdeeplearning\\lib\\site-packages\\tensorflow\\python\\training\\monitored_session.py\", line 551, in create_session\n    self.tf_sess = self._session_creator.create_session()\n  File \"C:\\Users\\I349732\\AppData\\Local\\Continuum\\anaconda3\\envs\\tfdeeplearning\\lib\\site-packages\\tensorflow\\python\\training\\monitored_session.py\", line 416, in create_session\n    self._scaffold.finalize()\n  File \"C:\\Users\\I349732\\AppData\\Local\\Continuum\\anaconda3\\envs\\tfdeeplearning\\lib\\site-packages\\tensorflow\\python\\training\\monitored_session.py\", line 209, in finalize\n    self._saver.build()\n  File \"C:\\Users\\I349732\\AppData\\Local\\Continuum\\anaconda3\\envs\\tfdeeplearning\\lib\\site-packages\\tensorflow\\python\\training\\saver.py\", line 1172, in build\n    filename=self._filename)\n  File \"C:\\Users\\I349732\\AppData\\Local\\Continuum\\anaconda3\\envs\\tfdeeplearning\\lib\\site-packages\\tensorflow\\python\\training\\saver.py\", line 682, in build\n    save_tensor = self._AddShardedSaveOps(filename_tensor, per_device)\n  File \"C:\\Users\\I349732\\AppData\\Local\\Continuum\\anaconda3\\envs\\tfdeeplearning\\lib\\site-packages\\tensorflow\\python\\training\\saver.py\", line 361, in _AddShardedSaveOps\n    return self._AddShardedSaveOpsForV2(filename_tensor, per_device)\n  File \"C:\\Users\\I349732\\AppData\\Local\\Continuum\\anaconda3\\envs\\tfdeeplearning\\lib\\site-packages\\tensorflow\\python\\training\\saver.py\", line 343, in _AddShardedSaveOpsForV2\n    sharded_prefixes, checkpoint_prefix, delete_old_dirs=True)\n  File \"C:\\Users\\I349732\\AppData\\Local\\Continuum\\anaconda3\\envs\\tfdeeplearning\\lib\\site-packages\\tensorflow\\python\\ops\\gen_io_ops.py\", line 208, in merge_v2_checkpoints\n    delete_old_dirs=delete_old_dirs, name=name)\n  File \"C:\\Users\\I349732\\AppData\\Local\\Continuum\\anaconda3\\envs\\tfdeeplearning\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py\", line 767, in apply_op\n    op_def=op_def)\n  File \"C:\\Users\\I349732\\AppData\\Local\\Continuum\\anaconda3\\envs\\tfdeeplearning\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\", line 2630, in create_op\n    original_op=self._default_original_op, op_def=op_def)\n  File \"C:\\Users\\I349732\\AppData\\Local\\Continuum\\anaconda3\\envs\\tfdeeplearning\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\", line 1204, in __init__\n    self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access\n\nUnknownError (see above for traceback): Failed to create a NewWriteableFile: perceptron_model\\model.ckpt-3501.index : Access is denied.\r\n; Input/output error\n\t [[Node: save/MergeV2Checkpoints = MergeV2Checkpoints[delete_old_dirs=true, _device=\"/job:localhost/replica:0/task:0/cpu:0\"](save/MergeV2Checkpoints/checkpoint_prefixes, _arg_save/Const_0_0)]]\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mUnknownError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\envs\\tfdeeplearning\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_call\u001b[1;34m(self, fn, *args)\u001b[0m\n\u001b[0;32m   1326\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1327\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1328\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\envs\\tfdeeplearning\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[1;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[0;32m   1305\u001b[0m                                    \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1306\u001b[1;33m                                    status, run_metadata)\n\u001b[0m\u001b[0;32m   1307\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\envs\\tfdeeplearning\\lib\\contextlib.py\u001b[0m in \u001b[0;36m__exit__\u001b[1;34m(self, type, value, traceback)\u001b[0m\n\u001b[0;32m     65\u001b[0m             \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 66\u001b[1;33m                 \u001b[0mnext\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgen\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     67\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\envs\\tfdeeplearning\\lib\\site-packages\\tensorflow\\python\\framework\\errors_impl.py\u001b[0m in \u001b[0;36mraise_exception_on_not_ok_status\u001b[1;34m()\u001b[0m\n\u001b[0;32m    465\u001b[0m           \u001b[0mcompat\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpywrap_tensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_Message\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstatus\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 466\u001b[1;33m           pywrap_tensorflow.TF_GetCode(status))\n\u001b[0m\u001b[0;32m    467\u001b[0m   \u001b[1;32mfinally\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mUnknownError\u001b[0m: Failed to create a NewWriteableFile: perceptron_model\\model.ckpt-3501.index : Access is denied.\r\n; Input/output error\n\t [[Node: save/MergeV2Checkpoints = MergeV2Checkpoints[delete_old_dirs=true, _device=\"/job:localhost/replica:0/task:0/cpu:0\"](save/MergeV2Checkpoints/checkpoint_prefixes, _arg_save/Const_0_0)]]",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mUnknownError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-21-6781a4ae384f>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0moutput_dim\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mclasses\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m run_experiment(x_train, y_train, x_test, y_test,\n\u001b[1;32m----> 5\u001b[1;33m                    bag_of_words_perceptron_model, 'train_and_evaluate', output_dim)\n\u001b[0m\u001b[0;32m      6\u001b[0m \u001b[0mtoc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-18-e856e4f3be98>\u001b[0m in \u001b[0;36mrun_experiment\u001b[1;34m(x_train, y_train, x_dev, y_dev, model_fn, schedule, output_dim, train_lengths, dev_lengths)\u001b[0m\n\u001b[0;32m     65\u001b[0m         \u001b[0mrun_config\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mrun_config\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     66\u001b[0m         \u001b[0mschedule\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mschedule\u001b[0m\u001b[1;33m,\u001b[0m  \u001b[1;31m# What to run, e.g. \"train_and_evaluate\", \"evaluate\", ...\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 67\u001b[1;33m         hparams=hparams)  # hyperparameters\n\u001b[0m",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\envs\\tfdeeplearning\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\learn_runner.py\u001b[0m in \u001b[0;36mrun\u001b[1;34m(experiment_fn, output_dir, schedule, run_config, hparams)\u001b[0m\n\u001b[0;32m    207\u001b[0m   \u001b[0mschedule\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mschedule\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0m_get_default_schedule\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_config\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    208\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 209\u001b[1;33m   \u001b[1;32mreturn\u001b[0m \u001b[0m_execute_schedule\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mexperiment\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mschedule\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    210\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    211\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\envs\\tfdeeplearning\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\learn_runner.py\u001b[0m in \u001b[0;36m_execute_schedule\u001b[1;34m(experiment, schedule)\u001b[0m\n\u001b[0;32m     44\u001b[0m     \u001b[0mlogging\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0merror\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Allowed values for this experiment are: %s'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalid_tasks\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     45\u001b[0m     \u001b[1;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Schedule references non-callable member %s'\u001b[0m \u001b[1;33m%\u001b[0m \u001b[0mschedule\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 46\u001b[1;33m   \u001b[1;32mreturn\u001b[0m \u001b[0mtask\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     47\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     48\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\envs\\tfdeeplearning\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\experiment.py\u001b[0m in \u001b[0;36mtrain_and_evaluate\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    500\u001b[0m             \u001b[0mname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0meval_dir_suffix\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhooks\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_eval_hooks\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    501\u001b[0m         )]\n\u001b[1;32m--> 502\u001b[1;33m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdelay_secs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    503\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    504\u001b[0m     eval_result = self._call_evaluate(input_fn=self._eval_input_fn,\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\envs\\tfdeeplearning\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\experiment.py\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(self, delay_secs)\u001b[0m\n\u001b[0;32m    278\u001b[0m     return self._call_train(input_fn=self._train_input_fn,\n\u001b[0;32m    279\u001b[0m                             \u001b[0mmax_steps\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_train_steps\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 280\u001b[1;33m                             hooks=self._train_monitors + extra_hooks)\n\u001b[0m\u001b[0;32m    281\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    282\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0mevaluate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdelay_secs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\envs\\tfdeeplearning\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\experiment.py\u001b[0m in \u001b[0;36m_call_train\u001b[1;34m(self, _sentinel, input_fn, steps, hooks, max_steps)\u001b[0m\n\u001b[0;32m    670\u001b[0m                                    \u001b[0msteps\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msteps\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    671\u001b[0m                                    \u001b[0mmax_steps\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmax_steps\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 672\u001b[1;33m                                    hooks=hooks)\n\u001b[0m\u001b[0;32m    673\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    674\u001b[0m       return self._estimator.fit(input_fn=input_fn,\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\envs\\tfdeeplearning\\lib\\site-packages\\tensorflow\\python\\estimator\\estimator.py\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(self, input_fn, hooks, steps, max_steps)\u001b[0m\n\u001b[0;32m    239\u001b[0m       \u001b[0mhooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtraining\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mStopAtStepHook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msteps\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmax_steps\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    240\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 241\u001b[1;33m     \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_train_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput_fn\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minput_fn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhooks\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mhooks\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    242\u001b[0m     \u001b[0mlogging\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Loss for final step: %s.'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    243\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\envs\\tfdeeplearning\\lib\\site-packages\\tensorflow\\python\\estimator\\estimator.py\u001b[0m in \u001b[0;36m_train_model\u001b[1;34m(self, input_fn, hooks)\u001b[0m\n\u001b[0;32m    684\u001b[0m         \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    685\u001b[0m         \u001b[1;32mwhile\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mmon_sess\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshould_stop\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 686\u001b[1;33m           \u001b[0m_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmon_sess\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mestimator_spec\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain_op\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mestimator_spec\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mloss\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    687\u001b[0m       \u001b[1;32mreturn\u001b[0m \u001b[0mloss\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    688\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\envs\\tfdeeplearning\\lib\\site-packages\\tensorflow\\python\\training\\monitored_session.py\u001b[0m in \u001b[0;36mrun\u001b[1;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m    516\u001b[0m                           \u001b[0mfeed_dict\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    517\u001b[0m                           \u001b[0moptions\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0moptions\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 518\u001b[1;33m                           run_metadata=run_metadata)\n\u001b[0m\u001b[0;32m    519\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    520\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0mshould_stop\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\envs\\tfdeeplearning\\lib\\site-packages\\tensorflow\\python\\training\\monitored_session.py\u001b[0m in \u001b[0;36mrun\u001b[1;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m    860\u001b[0m                               \u001b[0mfeed_dict\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    861\u001b[0m                               \u001b[0moptions\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0moptions\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 862\u001b[1;33m                               run_metadata=run_metadata)\n\u001b[0m\u001b[0;32m    863\u001b[0m       \u001b[1;32mexcept\u001b[0m \u001b[0m_PREEMPTION_ERRORS\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    864\u001b[0m         logging.info('An error was raised. This may be due to a preemption in '\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\envs\\tfdeeplearning\\lib\\site-packages\\tensorflow\\python\\training\\monitored_session.py\u001b[0m in \u001b[0;36mrun\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    816\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    817\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0mrun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 818\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_sess\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    819\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    820\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\envs\\tfdeeplearning\\lib\\site-packages\\tensorflow\\python\\training\\monitored_session.py\u001b[0m in \u001b[0;36mrun\u001b[1;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m    978\u001b[0m               \u001b[0mresults\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mhook\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mhook\u001b[0m \u001b[1;32min\u001b[0m \u001b[0moutputs\u001b[0m \u001b[1;32melse\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    979\u001b[0m               \u001b[0moptions\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0moptions\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 980\u001b[1;33m               run_metadata=run_metadata))\n\u001b[0m\u001b[0;32m    981\u001b[0m     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_should_stop\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_should_stop\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mrun_context\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstop_requested\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    982\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\envs\\tfdeeplearning\\lib\\site-packages\\tensorflow\\python\\training\\basic_session_run_hooks.py\u001b[0m in \u001b[0;36mafter_run\u001b[1;34m(self, run_context, run_values)\u001b[0m\n\u001b[0;32m    440\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_timer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshould_trigger_for_step\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mglobal_step\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    441\u001b[0m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_timer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate_last_triggered_step\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mglobal_step\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 442\u001b[1;33m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_save\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mglobal_step\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrun_context\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msession\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    443\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    444\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0mend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msession\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\envs\\tfdeeplearning\\lib\\site-packages\\tensorflow\\python\\training\\basic_session_run_hooks.py\u001b[0m in \u001b[0;36m_save\u001b[1;34m(self, step, session)\u001b[0m\n\u001b[0;32m    456\u001b[0m       \u001b[0ml\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbefore_save\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msession\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstep\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    457\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 458\u001b[1;33m     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_saver\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msave\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msession\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_save_path\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mglobal_step\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    459\u001b[0m     self._summary_writer.add_session_log(\n\u001b[0;32m    460\u001b[0m         SessionLog(\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\envs\\tfdeeplearning\\lib\\site-packages\\tensorflow\\python\\training\\saver.py\u001b[0m in \u001b[0;36msave\u001b[1;34m(self, sess, save_path, global_step, latest_filename, meta_graph_suffix, write_meta_graph, write_state)\u001b[0m\n\u001b[0;32m   1472\u001b[0m         model_checkpoint_path = sess.run(\n\u001b[0;32m   1473\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msaver_def\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msave_tensor_name\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1474\u001b[1;33m             {self.saver_def.filename_tensor_name: checkpoint_file})\n\u001b[0m\u001b[0;32m   1475\u001b[0m         \u001b[0mmodel_checkpoint_path\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_str\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel_checkpoint_path\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1476\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mwrite_state\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\envs\\tfdeeplearning\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36mrun\u001b[1;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m    893\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    894\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[1;32m--> 895\u001b[1;33m                          run_metadata_ptr)\n\u001b[0m\u001b[0;32m    896\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    897\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\envs\\tfdeeplearning\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run\u001b[1;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1122\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[1;32mor\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1123\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[1;32m-> 1124\u001b[1;33m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[0;32m   1125\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1126\u001b[0m       \u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\envs\\tfdeeplearning\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_run\u001b[1;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1319\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1320\u001b[0m       return self._do_call(_run_fn, self._session, feeds, fetches, targets,\n\u001b[1;32m-> 1321\u001b[1;33m                            options, run_metadata)\n\u001b[0m\u001b[0;32m   1322\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1323\u001b[0m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\envs\\tfdeeplearning\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_call\u001b[1;34m(self, fn, *args)\u001b[0m\n\u001b[0;32m   1338\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1339\u001b[0m           \u001b[1;32mpass\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1340\u001b[1;33m       \u001b[1;32mraise\u001b[0m \u001b[0mtype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnode_def\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mop\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1341\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1342\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_extend_graph\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mUnknownError\u001b[0m: Failed to create a NewWriteableFile: perceptron_model\\model.ckpt-3501.index : Access is denied.\r\n; Input/output error\n\t [[Node: save/MergeV2Checkpoints = MergeV2Checkpoints[delete_old_dirs=true, _device=\"/job:localhost/replica:0/task:0/cpu:0\"](save/MergeV2Checkpoints/checkpoint_prefixes, _arg_save/Const_0_0)]]\n\nCaused by op 'save/MergeV2Checkpoints', defined at:\n  File \"C:\\Users\\I349732\\AppData\\Local\\Continuum\\anaconda3\\envs\\tfdeeplearning\\lib\\runpy.py\", line 193, in _run_module_as_main\n    \"__main__\", mod_spec)\n  File \"C:\\Users\\I349732\\AppData\\Local\\Continuum\\anaconda3\\envs\\tfdeeplearning\\lib\\runpy.py\", line 85, in _run_code\n    exec(code, run_globals)\n  File \"C:\\Users\\I349732\\AppData\\Local\\Continuum\\anaconda3\\envs\\tfdeeplearning\\lib\\site-packages\\ipykernel_launcher.py\", line 16, in <module>\n    app.launch_new_instance()\n  File \"C:\\Users\\I349732\\AppData\\Local\\Continuum\\anaconda3\\envs\\tfdeeplearning\\lib\\site-packages\\traitlets\\config\\application.py\", line 658, in launch_instance\n    app.start()\n  File \"C:\\Users\\I349732\\AppData\\Local\\Continuum\\anaconda3\\envs\\tfdeeplearning\\lib\\site-packages\\ipykernel\\kernelapp.py\", line 477, in start\n    ioloop.IOLoop.instance().start()\n  File \"C:\\Users\\I349732\\AppData\\Local\\Continuum\\anaconda3\\envs\\tfdeeplearning\\lib\\site-packages\\zmq\\eventloop\\ioloop.py\", line 177, in start\n    super(ZMQIOLoop, self).start()\n  File \"C:\\Users\\I349732\\AppData\\Local\\Continuum\\anaconda3\\envs\\tfdeeplearning\\lib\\site-packages\\tornado\\ioloop.py\", line 888, in start\n    handler_func(fd_obj, events)\n  File \"C:\\Users\\I349732\\AppData\\Local\\Continuum\\anaconda3\\envs\\tfdeeplearning\\lib\\site-packages\\tornado\\stack_context.py\", line 277, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"C:\\Users\\I349732\\AppData\\Local\\Continuum\\anaconda3\\envs\\tfdeeplearning\\lib\\site-packages\\zmq\\eventloop\\zmqstream.py\", line 440, in _handle_events\n    self._handle_recv()\n  File \"C:\\Users\\I349732\\AppData\\Local\\Continuum\\anaconda3\\envs\\tfdeeplearning\\lib\\site-packages\\zmq\\eventloop\\zmqstream.py\", line 472, in _handle_recv\n    self._run_callback(callback, msg)\n  File \"C:\\Users\\I349732\\AppData\\Local\\Continuum\\anaconda3\\envs\\tfdeeplearning\\lib\\site-packages\\zmq\\eventloop\\zmqstream.py\", line 414, in _run_callback\n    callback(*args, **kwargs)\n  File \"C:\\Users\\I349732\\AppData\\Local\\Continuum\\anaconda3\\envs\\tfdeeplearning\\lib\\site-packages\\tornado\\stack_context.py\", line 277, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"C:\\Users\\I349732\\AppData\\Local\\Continuum\\anaconda3\\envs\\tfdeeplearning\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 283, in dispatcher\n    return self.dispatch_shell(stream, msg)\n  File \"C:\\Users\\I349732\\AppData\\Local\\Continuum\\anaconda3\\envs\\tfdeeplearning\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 235, in dispatch_shell\n    handler(stream, idents, msg)\n  File \"C:\\Users\\I349732\\AppData\\Local\\Continuum\\anaconda3\\envs\\tfdeeplearning\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 399, in execute_request\n    user_expressions, allow_stdin)\n  File \"C:\\Users\\I349732\\AppData\\Local\\Continuum\\anaconda3\\envs\\tfdeeplearning\\lib\\site-packages\\ipykernel\\ipkernel.py\", line 196, in do_execute\n    res = shell.run_cell(code, store_history=store_history, silent=silent)\n  File \"C:\\Users\\I349732\\AppData\\Local\\Continuum\\anaconda3\\envs\\tfdeeplearning\\lib\\site-packages\\ipykernel\\zmqshell.py\", line 533, in run_cell\n    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n  File \"C:\\Users\\I349732\\AppData\\Local\\Continuum\\anaconda3\\envs\\tfdeeplearning\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2698, in run_cell\n    interactivity=interactivity, compiler=compiler, result=result)\n  File \"C:\\Users\\I349732\\AppData\\Local\\Continuum\\anaconda3\\envs\\tfdeeplearning\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2802, in run_ast_nodes\n    if self.run_code(code, result):\n  File \"C:\\Users\\I349732\\AppData\\Local\\Continuum\\anaconda3\\envs\\tfdeeplearning\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2862, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"<ipython-input-21-6781a4ae384f>\", line 5, in <module>\n    bag_of_words_perceptron_model, 'train_and_evaluate', output_dim)\n  File \"<ipython-input-18-e856e4f3be98>\", line 67, in run_experiment\n    hparams=hparams)  # hyperparameters\n  File \"C:\\Users\\I349732\\AppData\\Local\\Continuum\\anaconda3\\envs\\tfdeeplearning\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\learn_runner.py\", line 209, in run\n    return _execute_schedule(experiment, schedule)\n  File \"C:\\Users\\I349732\\AppData\\Local\\Continuum\\anaconda3\\envs\\tfdeeplearning\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\learn_runner.py\", line 46, in _execute_schedule\n    return task()\n  File \"C:\\Users\\I349732\\AppData\\Local\\Continuum\\anaconda3\\envs\\tfdeeplearning\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\experiment.py\", line 502, in train_and_evaluate\n    self.train(delay_secs=0)\n  File \"C:\\Users\\I349732\\AppData\\Local\\Continuum\\anaconda3\\envs\\tfdeeplearning\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\experiment.py\", line 280, in train\n    hooks=self._train_monitors + extra_hooks)\n  File \"C:\\Users\\I349732\\AppData\\Local\\Continuum\\anaconda3\\envs\\tfdeeplearning\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\experiment.py\", line 672, in _call_train\n    hooks=hooks)\n  File \"C:\\Users\\I349732\\AppData\\Local\\Continuum\\anaconda3\\envs\\tfdeeplearning\\lib\\site-packages\\tensorflow\\python\\estimator\\estimator.py\", line 241, in train\n    loss = self._train_model(input_fn=input_fn, hooks=hooks)\n  File \"C:\\Users\\I349732\\AppData\\Local\\Continuum\\anaconda3\\envs\\tfdeeplearning\\lib\\site-packages\\tensorflow\\python\\estimator\\estimator.py\", line 683, in _train_model\n    log_step_count_steps=self._config.log_step_count_steps) as mon_sess:\n  File \"C:\\Users\\I349732\\AppData\\Local\\Continuum\\anaconda3\\envs\\tfdeeplearning\\lib\\site-packages\\tensorflow\\python\\training\\monitored_session.py\", line 365, in MonitoredTrainingSession\n    stop_grace_period_secs=stop_grace_period_secs)\n  File \"C:\\Users\\I349732\\AppData\\Local\\Continuum\\anaconda3\\envs\\tfdeeplearning\\lib\\site-packages\\tensorflow\\python\\training\\monitored_session.py\", line 668, in __init__\n    stop_grace_period_secs=stop_grace_period_secs)\n  File \"C:\\Users\\I349732\\AppData\\Local\\Continuum\\anaconda3\\envs\\tfdeeplearning\\lib\\site-packages\\tensorflow\\python\\training\\monitored_session.py\", line 490, in __init__\n    self._sess = _RecoverableSession(self._coordinated_creator)\n  File \"C:\\Users\\I349732\\AppData\\Local\\Continuum\\anaconda3\\envs\\tfdeeplearning\\lib\\site-packages\\tensorflow\\python\\training\\monitored_session.py\", line 842, in __init__\n    _WrappedSession.__init__(self, self._create_session())\n  File \"C:\\Users\\I349732\\AppData\\Local\\Continuum\\anaconda3\\envs\\tfdeeplearning\\lib\\site-packages\\tensorflow\\python\\training\\monitored_session.py\", line 847, in _create_session\n    return self._sess_creator.create_session()\n  File \"C:\\Users\\I349732\\AppData\\Local\\Continuum\\anaconda3\\envs\\tfdeeplearning\\lib\\site-packages\\tensorflow\\python\\training\\monitored_session.py\", line 551, in create_session\n    self.tf_sess = self._session_creator.create_session()\n  File \"C:\\Users\\I349732\\AppData\\Local\\Continuum\\anaconda3\\envs\\tfdeeplearning\\lib\\site-packages\\tensorflow\\python\\training\\monitored_session.py\", line 416, in create_session\n    self._scaffold.finalize()\n  File \"C:\\Users\\I349732\\AppData\\Local\\Continuum\\anaconda3\\envs\\tfdeeplearning\\lib\\site-packages\\tensorflow\\python\\training\\monitored_session.py\", line 209, in finalize\n    self._saver.build()\n  File \"C:\\Users\\I349732\\AppData\\Local\\Continuum\\anaconda3\\envs\\tfdeeplearning\\lib\\site-packages\\tensorflow\\python\\training\\saver.py\", line 1172, in build\n    filename=self._filename)\n  File \"C:\\Users\\I349732\\AppData\\Local\\Continuum\\anaconda3\\envs\\tfdeeplearning\\lib\\site-packages\\tensorflow\\python\\training\\saver.py\", line 682, in build\n    save_tensor = self._AddShardedSaveOps(filename_tensor, per_device)\n  File \"C:\\Users\\I349732\\AppData\\Local\\Continuum\\anaconda3\\envs\\tfdeeplearning\\lib\\site-packages\\tensorflow\\python\\training\\saver.py\", line 361, in _AddShardedSaveOps\n    return self._AddShardedSaveOpsForV2(filename_tensor, per_device)\n  File \"C:\\Users\\I349732\\AppData\\Local\\Continuum\\anaconda3\\envs\\tfdeeplearning\\lib\\site-packages\\tensorflow\\python\\training\\saver.py\", line 343, in _AddShardedSaveOpsForV2\n    sharded_prefixes, checkpoint_prefix, delete_old_dirs=True)\n  File \"C:\\Users\\I349732\\AppData\\Local\\Continuum\\anaconda3\\envs\\tfdeeplearning\\lib\\site-packages\\tensorflow\\python\\ops\\gen_io_ops.py\", line 208, in merge_v2_checkpoints\n    delete_old_dirs=delete_old_dirs, name=name)\n  File \"C:\\Users\\I349732\\AppData\\Local\\Continuum\\anaconda3\\envs\\tfdeeplearning\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py\", line 767, in apply_op\n    op_def=op_def)\n  File \"C:\\Users\\I349732\\AppData\\Local\\Continuum\\anaconda3\\envs\\tfdeeplearning\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\", line 2630, in create_op\n    original_op=self._default_original_op, op_def=op_def)\n  File \"C:\\Users\\I349732\\AppData\\Local\\Continuum\\anaconda3\\envs\\tfdeeplearning\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\", line 1204, in __init__\n    self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access\n\nUnknownError (see above for traceback): Failed to create a NewWriteableFile: perceptron_model\\model.ckpt-3501.index : Access is denied.\r\n; Input/output error\n\t [[Node: save/MergeV2Checkpoints = MergeV2Checkpoints[delete_old_dirs=true, _device=\"/job:localhost/replica:0/task:0/cpu:0\"](save/MergeV2Checkpoints/checkpoint_prefixes, _arg_save/Const_0_0)]]\n"
     ]
    }
   ],
   "source": [
    "tic()\n",
    "train_raw, x_train, y_train, x_test, y_test, _, _, classes = preprocess_data()\n",
    "output_dim = len(classes)\n",
    "run_experiment(x_train, y_train, x_test, y_test,\n",
    "                   bag_of_words_perceptron_model, 'train_and_evaluate', output_dim)\n",
    "toc()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
