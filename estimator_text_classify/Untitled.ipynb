{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from os import path, makedirs, rename, remove\n",
    "from time import time\n",
    "import argparse\n",
    "import pickle\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from sklearn.utils import shuffle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "DATA_DIRECTORY = 'data'\n",
    "MAX_DOCUMENT_LENGTH = 10\n",
    "MAX_VOCABULARY_SIZE = 1000000\n",
    "EMBEDDING_DIM = 25\n",
    "TF_SEED = 4242\n",
    "NP_SEED = 1234\n",
    "CHECKPOINTS_PER_EPOCH = 5\n",
    "WORD_METADATA_FILENAME = 'word_metadata.tsv'\n",
    "VOCAB_PROCESSOR_FILENAME = 'vocab_processor.pickle'\n",
    "DATA_FILENAME = 'data.pickle'\n",
    "VERBOSITY = 'info'\n",
    "WORDS_FEATURE = 'words'  # Name of the input words feature.\n",
    "LENGTHS_FEATURE = 'lengths'  # Name of the document lengths feature (not used for BOW)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Timing functions (MATLAB style)\n",
    "\"\"\"\n",
    "_tstart_stack = []\n",
    "def tic():\n",
    "    _tstart_stack.append(time())\n",
    "\n",
    "\n",
    "def toc(fmt=\"Elapsed: %.2f s\"):\n",
    "    print(fmt % (time() - _tstart_stack.pop()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_data(data_directory, classes_only=False):\n",
    "    \"\"\"Download the DBpedia data if necessary, and load data from the data_directory. If the files train.csv, test.csv\n",
    "       and classes.txt are all in data_directory, then they are used (no download).\"\"\"\n",
    "    # The function call load_dataset in the TensorFlow API is supposed to provide this functionality. However, there are\n",
    "    # currently issues: https://github.com/tensorflow/tensorflow/issues/14698\n",
    "\n",
    "    train_filename = path.join(data_directory, 'train.csv')\n",
    "    test_filename = path.join(data_directory, 'test.csv')\n",
    "    classes_filename = path.join(data_directory, 'classes.txt')\n",
    "    has_train = path.isfile(train_filename)\n",
    "    has_test = path.isfile(test_filename)\n",
    "    has_classes = path.isfile(classes_filename)\n",
    "\n",
    "    if not has_train or not has_test or not has_classes:\n",
    "        # Download the data if necessary, using the API.\n",
    "        tf.contrib.learn.datasets.text_datasets.maybe_download_dbpedia(data_directory)\n",
    "        csv_subdir = 'dbpedia_csv'\n",
    "\n",
    "        if has_train:\n",
    "            remove(train_filename)\n",
    "        rename(path.join(data_directory, csv_subdir, 'train.csv'), train_filename)\n",
    "        if has_test:\n",
    "            remove(test_filename)\n",
    "        rename(path.join(data_directory, csv_subdir, 'test.csv'), test_filename)\n",
    "        if has_classes:\n",
    "            remove(classes_filename)\n",
    "        rename(path.join(data_directory, csv_subdir, 'classes.txt'), classes_filename)\n",
    "\n",
    "    classes = pd.read_csv(classes_filename, header=None, names=['class'])\n",
    "    if classes_only:\n",
    "        return classes\n",
    "    train_raw = pd.read_csv(train_filename, header=None)\n",
    "    test_raw = pd.read_csv(test_filename, header=None)\n",
    "    longest_sent = max([len(sent) for sent in tf.contrib.learn.preprocessing.tokenizer(train_raw[2])])\n",
    "    print(\"The longest sentence in the training data has {} words.\".format(longest_sent))\n",
    "\n",
    "    return train_raw, test_raw, classes\n",
    "\n",
    "\n",
    "def extract_data(train_raw, test_raw):\n",
    "    \"\"\"Extract the document and class from each entry in the data.\"\"\"\n",
    "    x_train = train_raw[2]\n",
    "    y_train = train_raw[0] - 1  # Start enumeration at 0 instead of 1\n",
    "    x_test = test_raw[2]\n",
    "    y_test = test_raw[0] - 1\n",
    "    print('Size of training set: {0}'.format(len(x_train)))\n",
    "    print('Size of test set: {0}'.format(len(x_test)))\n",
    "    return x_train, np.array(y_train), x_test, np.array(y_test)\n",
    "\n",
    "def process_vocabulary(train_sentences, test_sentences, flags,\n",
    "                       reuse=True, vocabulary_processor=None, extend=False, sequence_lengths=False):\n",
    "    \"\"\"Map words to integers, and then map sentences to integer sequences of length flags.max_doc_len, by truncating and\n",
    "       padding as needed. This leads to an integer matrix of data which is what TensorFlow can work with. The processor\n",
    "       is then saved to disk in a file determined by flags.\n",
    "\n",
    "    Args:\n",
    "       reuse: if True load the vocabulary_processor is loaded from disk if the file exists.\n",
    "       vocabulary_processor: if not None, and it was not loaded from disk, the passed vocabulary_processor is used.\n",
    "       extend: if True the vocabulary processor (loaded or passed) is extended.\n",
    "       sequence_lengths: Whether to list the length of each document.\n",
    "    \"\"\"\n",
    "    \n",
    "    vocabulary_processor_path = path.join(flags.model_dir, flags.vocab_processor_file)\n",
    "    # If vocabulary_processor gets created/altered save it.\n",
    "    if reuse and path.isfile(vocabulary_processor_path):\n",
    "        vocabulary_processor = tf.contrib.learn.preprocessing.VocabularyProcessor.restore(vocabulary_processor_path)\n",
    "        save_vocab_processor = extend\n",
    "    elif vocabulary_processor is None:\n",
    "        vocabulary_processor = tf.contrib.learn.preprocessing.VocabularyProcessor(flags.max_doc_len)\n",
    "        vocabulary_processor.fit(train_sentences)\n",
    "        save_vocab_processor = True\n",
    "    elif extend:\n",
    "        vocabulary_processor.vocabulary_.freeze(False)\n",
    "        vocabulary_processor.fit(train_sentences)\n",
    "        save_vocab_processor = True\n",
    "    else:\n",
    "        save_vocab_processor = False\n",
    "\n",
    "    if train_sentences is not None:\n",
    "        train_bow = np.array(list(vocabulary_processor.transform(train_sentences)))\n",
    "    else:\n",
    "        train_bow = None\n",
    "    if test_sentences is not None:\n",
    "        test_bow = np.array(list(vocabulary_processor.transform(test_sentences)))\n",
    "    else:\n",
    "        test_bow = None\n",
    "    n_words = len(vocabulary_processor.vocabulary_)\n",
    "    print('Number of words in vocabulary: %d' % n_words)\n",
    "\n",
    "    if save_vocab_processor:\n",
    "        if not path.isdir(flags.model_dir):\n",
    "            makedirs(flags.model_dir)\n",
    "        vocabulary_processor.save(vocabulary_processor_path)\n",
    "\n",
    "    if sequence_lengths:\n",
    "        def calculate_lengths(arr):\n",
    "            return arr.shape[1] - (arr != 0)[:, ::-1].argmax(axis=1)\n",
    "        train_lengths = calculate_lengths(train_bow) if train_bow is not None else None\n",
    "        test_lengths = calculate_lengths(test_bow) if test_bow is not None else None\n",
    "    else:\n",
    "        train_lengths = test_lengths = None\n",
    "\n",
    "    return train_bow, test_bow, train_lengths, test_lengths, vocabulary_processor, n_words\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def preprocess_data(flags, sequence_lengths=False):\n",
    "    '''\n",
    "    Load data, shuffle it, process the vocabulary and save to DATA_FILENAME, if not done already.\n",
    "    Returns processed data. NOTE: If the max_doc_len changes from a previous run,\n",
    "    then DATA_FILENAME should be deleted so that it can be properly recreated.\n",
    "    '''\n",
    "    preprocessed_path = path.join(flags.model_dir, DATA_FILENAME)\n",
    "    if path.isfile(preprocessed_path):\n",
    "        with open(preprocessed_path, 'rb') as f:\n",
    "            train_raw, x_train, y_train, x_test, y_test, \\\n",
    "            train_lengths, test_lengths, classes = pickle.load(f)\n",
    "    else:\n",
    "        #Get the raw data, downloading if neccessary \n",
    "        train_raw, test_raw, classes = get_data(flags.data_dir)\n",
    "        \n",
    "        #Seeding is neccessary for reproducability\n",
    "        np.random.seed(flags.np_seed)\n",
    "        \n",
    "        # Shuffle data to make the distribution of classes roughly stratified for each mini-batch.\n",
    "        # This is not necessary for full batch training, but is essential for mini-batch training.\n",
    "        train_raw = shuffle(train_raw)\n",
    "        test_raw = shuffle(test_raw)\n",
    "        train_sentences, y_train, test_sentences, y_test = extract_data(train_raw, test_raw)\n",
    "        # Encode the raw data as integer vectors.\n",
    "        x_train, x_test, train_lengths, test_lengths, _, _ = process_vocabulary(\n",
    "            train_sentences, test_sentences, flags,\n",
    "            reuse=True, sequence_lengths=sequence_lengths)\n",
    "        # Save the processed data to avoid re-processing.\n",
    "        saved = False\n",
    "        with open(preprocessed_path, 'wb') as f:\n",
    "            try:\n",
    "                pickle.dump([train_raw, x_train, y_train, x_test, y_test,\n",
    "                             train_lengths, test_lengths, classes], f)\n",
    "                saved = True\n",
    "            except (OverflowError, MemoryError):\n",
    "                # Can happen if max-doc-len is large.\n",
    "                pass\n",
    "\n",
    "        if not saved:\n",
    "            remove(preprocessed_path)\n",
    "\n",
    "    return train_raw, x_train, y_train, x_test, y_test, train_lengths, test_lengths, classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
