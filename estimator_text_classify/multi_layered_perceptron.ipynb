{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from os import path, makedirs, rename, remove\n",
    "from time import time\n",
    "import argparse\n",
    "import pickle\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from sklearn.utils import shuffle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "DATA_DIRECTORY = 'data'\n",
    "MAX_DOCUMENT_LENGTH = 10\n",
    "MAX_VOCABULARY_SIZE = 1000000\n",
    "EMBEDDING_DIM = 25\n",
    "TF_SEED = 4242\n",
    "MODEL_DIRECTORY = 'perceptron_mlp_model'\n",
    "NUM_EPOCHS = 2\n",
    "BATCH_SIZE = 32\n",
    "LEARNING_RATE = 0.04\n",
    "NP_SEED = 1234\n",
    "CHECKPOINTS_PER_EPOCH = 5\n",
    "WORD_METADATA_FILENAME = 'word_metadata.tsv'\n",
    "VOCAB_PROCESSOR_FILENAME = 'vocab_processor.pickle'\n",
    "DATA_FILENAME = 'data.pickle'\n",
    "VERBOSITY = 'info'\n",
    "WORDS_FEATURE = 'words'  # Name of the input words feature.\n",
    "LENGTHS_FEATURE = 'lengths'  # Name of the document lengths feature (not used for BOW)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Timing functions (MATLAB style)\n",
    "\"\"\"\n",
    "_tstart_stack = []\n",
    "def tic():\n",
    "    _tstart_stack.append(time())\n",
    "\n",
    "\n",
    "def toc(fmt=\"Elapsed: %.2f s\"):\n",
    "    print(fmt % (time() - _tstart_stack.pop()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_data(data_directory, classes_only=False):\n",
    "    \"\"\"Download the DBpedia data if necessary, and load data from the data_directory. If the files train.csv, test.csv\n",
    "       and classes.txt are all in data_directory, then they are used (no download).\"\"\"\n",
    "    # The function call load_dataset in the TensorFlow API is supposed to provide this functionality. However, there are\n",
    "    # currently issues: https://github.com/tensorflow/tensorflow/issues/14698\n",
    "\n",
    "    train_filename = path.join(data_directory, 'train.csv')\n",
    "    test_filename = path.join(data_directory, 'test.csv')\n",
    "    classes_filename = path.join(data_directory, 'classes.txt')\n",
    "    has_train = path.isfile(train_filename)\n",
    "    has_test = path.isfile(test_filename)\n",
    "    has_classes = path.isfile(classes_filename)\n",
    "\n",
    "    if not has_train or not has_test or not has_classes:\n",
    "        # Download the data if necessary, using the API.\n",
    "        tf.contrib.learn.datasets.text_datasets.maybe_download_dbpedia(data_directory)\n",
    "        csv_subdir = 'dbpedia_csv'\n",
    "\n",
    "        if has_train:\n",
    "            remove(train_filename)\n",
    "        rename(path.join(data_directory, csv_subdir, 'train.csv'), train_filename)\n",
    "        if has_test:\n",
    "            remove(test_filename)\n",
    "        rename(path.join(data_directory, csv_subdir, 'test.csv'), test_filename)\n",
    "        if has_classes:\n",
    "            remove(classes_filename)\n",
    "        rename(path.join(data_directory, csv_subdir, 'classes.txt'), classes_filename)\n",
    "\n",
    "    classes = pd.read_csv(classes_filename, header=None, names=['class'])\n",
    "    if classes_only:\n",
    "        return classes\n",
    "    train_raw = pd.read_csv(train_filename, header=None)\n",
    "    test_raw = pd.read_csv(test_filename, header=None)\n",
    "    longest_sent = max([len(sent) for sent in tf.contrib.learn.preprocessing.tokenizer(train_raw[2])])\n",
    "    print(\"The longest sentence in the training data has {} words.\".format(longest_sent))\n",
    "\n",
    "    return train_raw, test_raw, classes\n",
    "\n",
    "\n",
    "def extract_data(train_raw, test_raw):\n",
    "    \"\"\"Extract the document and class from each entry in the data.\"\"\"\n",
    "    x_train = train_raw[2]\n",
    "    y_train = train_raw[0] - 1  # Start enumeration at 0 instead of 1\n",
    "    x_test = test_raw[2]\n",
    "    y_test = test_raw[0] - 1\n",
    "    print('Size of training set: {0}'.format(len(x_train)))\n",
    "    print('Size of test set: {0}'.format(len(x_test)))\n",
    "    return x_train, np.array(y_train), x_test, np.array(y_test)\n",
    "\n",
    "def process_vocabulary(train_sentences, test_sentences,\n",
    "                       reuse=True, vocabulary_processor=None, extend=False, sequence_lengths=False):\n",
    "    \"\"\"Map words to integers, and then map sentences to integer sequences of length flags.max_doc_len, by truncating and\n",
    "       padding as needed. This leads to an integer matrix of data which is what TensorFlow can work with. The processor\n",
    "       is then saved to disk in a file determined by flags.\n",
    "\n",
    "    Args:\n",
    "       reuse: if True load the vocabulary_processor is loaded from disk if the file exists.\n",
    "       vocabulary_processor: if not None, and it was not loaded from disk, the passed vocabulary_processor is used.\n",
    "       extend: if True the vocabulary processor (loaded or passed) is extended.\n",
    "       sequence_lengths: Whether to list the length of each document.\n",
    "    \"\"\"\n",
    "    \n",
    "    vocabulary_processor_path = path.join(MODEL_DIRECTORY, VOCAB_PROCESSOR_FILENAME)\n",
    "    # If vocabulary_processor gets created/altered save it.\n",
    "    if reuse and path.isfile(vocabulary_processor_path):\n",
    "        vocabulary_processor = tf.contrib.learn.preprocessing.VocabularyProcessor.restore(vocabulary_processor_path)\n",
    "        save_vocab_processor = extend\n",
    "    elif vocabulary_processor is None:\n",
    "        vocabulary_processor = tf.contrib.learn.preprocessing.VocabularyProcessor(MAX_DOCUMENT_LENGTH)\n",
    "        vocabulary_processor.fit(train_sentences)\n",
    "        save_vocab_processor = True\n",
    "    elif extend:\n",
    "        vocabulary_processor.vocabulary_.freeze(False)\n",
    "        vocabulary_processor.fit(train_sentences)\n",
    "        save_vocab_processor = True\n",
    "    else:\n",
    "        save_vocab_processor = False\n",
    "\n",
    "    if train_sentences is not None:\n",
    "        train_bow = np.array(list(vocabulary_processor.transform(train_sentences)))\n",
    "    else:\n",
    "        train_bow = None\n",
    "    if test_sentences is not None:\n",
    "        test_bow = np.array(list(vocabulary_processor.transform(test_sentences)))\n",
    "    else:\n",
    "        test_bow = None\n",
    "    n_words = len(vocabulary_processor.vocabulary_)\n",
    "    print('Number of words in vocabulary: %d' % n_words)\n",
    "\n",
    "    if save_vocab_processor:\n",
    "        if not path.isdir(MODEL_DIRECTORY):\n",
    "            makedirs(MODEL_DIRECTORY)\n",
    "        vocabulary_processor.save(vocabulary_processor_path)\n",
    "\n",
    "    if sequence_lengths:\n",
    "        def calculate_lengths(arr):\n",
    "            return arr.shape[1] - (arr != 0)[:, ::-1].argmax(axis=1)\n",
    "        train_lengths = calculate_lengths(train_bow) if train_bow is not None else None\n",
    "        test_lengths = calculate_lengths(test_bow) if test_bow is not None else None\n",
    "    else:\n",
    "        train_lengths = test_lengths = None\n",
    "\n",
    "    return train_bow, test_bow, train_lengths, test_lengths, vocabulary_processor, n_words\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def preprocess_data(sequence_lengths=False):\n",
    "    '''\n",
    "    Load data, shuffle it, process the vocabulary and save to DATA_FILENAME, if not done already.\n",
    "    Returns processed data. NOTE: If the max_doc_len changes from a previous run,\n",
    "    then DATA_FILENAME should be deleted so that it can be properly recreated.\n",
    "    '''\n",
    "    preprocessed_path = path.join(MODEL_DIRECTORY, DATA_FILENAME)\n",
    "    if path.isfile(preprocessed_path):\n",
    "        with open(preprocessed_path, 'rb') as f:\n",
    "            train_raw, x_train, y_train, x_test, y_test, \\\n",
    "            train_lengths, test_lengths, classes = pickle.load(f)\n",
    "    else:\n",
    "        #Get the raw data, downloading if neccessary \n",
    "        train_raw, test_raw, classes = get_data(DATA_DIRECTORY)\n",
    "        \n",
    "        #Seeding is neccessary for reproducability\n",
    "        np.random.seed(TF_SEED)\n",
    "        \n",
    "        # Shuffle data to make the distribution of classes roughly stratified for each mini-batch.\n",
    "        # This is not necessary for full batch training, but is essential for mini-batch training.\n",
    "        train_raw = shuffle(train_raw)\n",
    "        test_raw = shuffle(test_raw)\n",
    "        train_sentences, y_train, test_sentences, y_test = extract_data(train_raw, test_raw)\n",
    "        # Encode the raw data as integer vectors.\n",
    "        x_train, x_test, train_lengths, test_lengths, _, _ = process_vocabulary(\n",
    "            train_sentences, test_sentences,\n",
    "            reuse=True, sequence_lengths=sequence_lengths)\n",
    "        # Save the processed data to avoid re-processing.\n",
    "        saved = False\n",
    "        with open(preprocessed_path, 'wb') as f:\n",
    "            try:\n",
    "                pickle.dump([train_raw, x_train, y_train, x_test, y_test,\n",
    "                             train_lengths, test_lengths, classes], f)\n",
    "                saved = True\n",
    "            except (OverflowError, MemoryError):\n",
    "                # Can happen if max-doc-len is large.\n",
    "                pass\n",
    "\n",
    "        if not saved:\n",
    "            remove(preprocessed_path)\n",
    "\n",
    "    return train_raw, x_train, y_train, x_test, y_test, train_lengths, test_lengths, classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Modelling: Training, evaluation and prediction. Also metadata for TensorBoard.\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "def input_fn(x, y=None, lengths=None, batch_size=None, num_epochs=None, shuffle=False):\n",
    "    \"\"\"Generic input function to be used as the input_fn arguments for Experiment or directly with Estimators.\"\"\"\n",
    "    if batch_size is None and x is not None:\n",
    "        batch_size = len(x)\n",
    "    x_dict = {WORDS_FEATURE: x}\n",
    "    if lengths is not None:\n",
    "        x_dict['LENGTHS_FEATURE'] = lengths\n",
    "    return tf.estimator.inputs.numpy_input_fn(\n",
    "        x_dict,\n",
    "        y,\n",
    "        batch_size=batch_size,\n",
    "        num_epochs=num_epochs,\n",
    "        shuffle=shuffle)\n",
    "\n",
    "\n",
    "def run_experiment(x_train, y_train, x_dev, y_dev, model_fn, schedule, output_dim, train_lengths=None, dev_lengths=None):\n",
    "    \"\"\"Create experiment object and run it.\"\"\"\n",
    "    hparams = tf.contrib.training.HParams(\n",
    "        n_words=MAX_VOCABULARY_SIZE,\n",
    "        n_epochs=NUM_EPOCHS,\n",
    "        seed=TF_SEED,\n",
    "        batch_size=BATCH_SIZE,\n",
    "        learning_rate=LEARNING_RATE,\n",
    "        output_dim=output_dim,\n",
    "        embed_dim=50)\n",
    "\n",
    "    is_training = schedule in ['train', 'train_and_evaluate']\n",
    "    run_config = tf.contrib.learn.RunConfig()\n",
    "    try:\n",
    "        checkpoint_steps = len(x_train) / CHECKPOINTS_PER_EPOCH / BATCH_SIZE if is_training else None\n",
    "        log_step_count_steps = 100  # default value\n",
    "    except TypeError:\n",
    "        # Happens if batch_size is None\n",
    "        checkpoint_steps = 1\n",
    "        log_step_count_steps = 1\n",
    "    run_config = run_config.replace(model_dir=MODEL_DIRECTORY,\n",
    "                                    save_checkpoints_steps=checkpoint_steps,\n",
    "                                    log_step_count_steps=log_step_count_steps,\n",
    "                                    tf_random_seed=hparams.seed)\n",
    "\n",
    "    def experiment_fn(run_config, hparams):\n",
    "        estimator = tf.estimator.Estimator(\n",
    "            model_fn=model_fn,\n",
    "            config=run_config,\n",
    "            params=hparams)\n",
    "        experiment = tf.contrib.learn.Experiment(\n",
    "            estimator=estimator,\n",
    "            train_input_fn=input_fn(x_train, y_train, train_lengths,\n",
    "                                    batch_size=hparams.batch_size,\n",
    "                                    num_epochs=hparams.n_epochs,\n",
    "                                    shuffle=True),\n",
    "            eval_input_fn=input_fn(x_dev, y_dev, dev_lengths,\n",
    "                                   num_epochs=1),\n",
    "            eval_delay_secs=0)\n",
    "        return experiment\n",
    "\n",
    "    if is_training:\n",
    "        print('Training model for {} epochs...'.format(hparams.n_epochs))\n",
    "    tf.contrib.learn.learn_runner.run(\n",
    "        experiment_fn=experiment_fn,\n",
    "        run_config=run_config,\n",
    "        schedule=schedule,  # What to run, e.g. \"train_and_evaluate\", \"evaluate\", ...\n",
    "        hparams=hparams)  # hyperparameters\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def estimator_spec_for_softmax_classification(logits, labels, mode, params):\n",
    "    \"\"\"Returns EstimatorSpec instance for softmax classification.\"\"\"\n",
    "    predicted_class = tf.argmax(logits, 1)\n",
    "    if mode == tf.estimator.ModeKeys.PREDICT:\n",
    "        return tf.estimator.EstimatorSpec(\n",
    "            mode=mode,\n",
    "            predictions={\n",
    "                'class': predicted_class,\n",
    "                'prob': tf.nn.softmax(logits)\n",
    "            })\n",
    "\n",
    "    loss = tf.losses.sparse_softmax_cross_entropy(labels=labels, logits=logits)\n",
    "    if mode == tf.estimator.ModeKeys.TRAIN:\n",
    "        with tf.name_scope('OptimizeLoss'):\n",
    "            optimizer = tf.train.AdamOptimizer(learning_rate=params.learning_rate)\n",
    "            train_op = optimizer.minimize(loss, global_step=tf.train.get_global_step())\n",
    "        return tf.estimator.EstimatorSpec(mode=mode, loss=loss, train_op=train_op)\n",
    "\n",
    "    # mode == EVAL\n",
    "    eval_metric_ops = {\n",
    "        'accuracy': tf.metrics.accuracy(labels=labels, predictions=predicted_class)\n",
    "    }\n",
    "    return tf.estimator.EstimatorSpec(mode=mode, loss=loss, eval_metric_ops=eval_metric_ops)\n",
    "\n",
    "def bag_of_words_MLP_model(features, labels, mode, params):\n",
    "    \"\"\"MLP architecture\"\"\"\n",
    "    with tf.variable_scope('MLP'):\n",
    "        bow_column = tf.feature_column.categorical_column_with_identity(\n",
    "            WORDS_FEATURE, num_buckets=params.n_words)\n",
    "        bow_embedding_column = tf.feature_column.embedding_column(\n",
    "            bow_column, dimension=params.embed_dim)\n",
    "        bow = tf.feature_column.input_layer(\n",
    "            features,\n",
    "            feature_columns=[bow_embedding_column])\n",
    "        bow_activated = tf.nn.relu(bow)\n",
    "        logits = tf.layers.dense(bow_activated, params.output_dim, activation=None)\n",
    "\n",
    "    return estimator_spec_for_softmax_classification(logits, labels, mode, params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tic()\n",
    "train_raw, x_train, y_train, x_test, y_test, _, _, classes = preprocess_data()\n",
    "output_dim = len(classes)\n",
    "run_experiment(x_train, y_train, x_test, y_test,\n",
    "                   bag_of_words_MLP_model, 'train_and_evaluate', output_dim)\n",
    "toc()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
