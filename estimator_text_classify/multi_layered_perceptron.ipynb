{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from os import path, makedirs, rename, remove\n",
    "from time import time\n",
    "import argparse\n",
    "import pickle\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from sklearn.utils import shuffle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "DATA_DIRECTORY = 'data'\n",
    "MAX_DOCUMENT_LENGTH = 10\n",
    "MAX_VOCABULARY_SIZE = 1000000\n",
    "EMBEDDING_DIM = 25\n",
    "TF_SEED = 4242\n",
    "MODEL_DIRECTORY = 'perceptron_mlp_model'\n",
    "NUM_EPOCHS = 2\n",
    "BATCH_SIZE = 32\n",
    "LEARNING_RATE = 0.04\n",
    "NP_SEED = 1234\n",
    "CHECKPOINTS_PER_EPOCH = 5\n",
    "WORD_METADATA_FILENAME = 'word_metadata.tsv'\n",
    "VOCAB_PROCESSOR_FILENAME = 'vocab_processor.pickle'\n",
    "DATA_FILENAME = 'data.pickle'\n",
    "VERBOSITY = 'info'\n",
    "WORDS_FEATURE = 'words'  # Name of the input words feature.\n",
    "LENGTHS_FEATURE = 'lengths'  # Name of the document lengths feature (not used for BOW)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Timing functions (MATLAB style)\n",
    "\"\"\"\n",
    "_tstart_stack = []\n",
    "def tic():\n",
    "    _tstart_stack.append(time())\n",
    "\n",
    "\n",
    "def toc(fmt=\"Elapsed: %.2f s\"):\n",
    "    print(fmt % (time() - _tstart_stack.pop()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_data(data_directory, classes_only=False):\n",
    "    \"\"\"Download the DBpedia data if necessary, and load data from the data_directory. If the files train.csv, test.csv\n",
    "       and classes.txt are all in data_directory, then they are used (no download).\"\"\"\n",
    "    # The function call load_dataset in the TensorFlow API is supposed to provide this functionality. However, there are\n",
    "    # currently issues: https://github.com/tensorflow/tensorflow/issues/14698\n",
    "\n",
    "    train_filename = path.join(data_directory, 'train.csv')\n",
    "    test_filename = path.join(data_directory, 'test.csv')\n",
    "    classes_filename = path.join(data_directory, 'classes.txt')\n",
    "    has_train = path.isfile(train_filename)\n",
    "    has_test = path.isfile(test_filename)\n",
    "    has_classes = path.isfile(classes_filename)\n",
    "\n",
    "    if not has_train or not has_test or not has_classes:\n",
    "        # Download the data if necessary, using the API.\n",
    "        tf.contrib.learn.datasets.text_datasets.maybe_download_dbpedia(data_directory)\n",
    "        csv_subdir = 'dbpedia_csv'\n",
    "\n",
    "        if has_train:\n",
    "            remove(train_filename)\n",
    "        rename(path.join(data_directory, csv_subdir, 'train.csv'), train_filename)\n",
    "        if has_test:\n",
    "            remove(test_filename)\n",
    "        rename(path.join(data_directory, csv_subdir, 'test.csv'), test_filename)\n",
    "        if has_classes:\n",
    "            remove(classes_filename)\n",
    "        rename(path.join(data_directory, csv_subdir, 'classes.txt'), classes_filename)\n",
    "\n",
    "    classes = pd.read_csv(classes_filename, header=None, names=['class'])\n",
    "    if classes_only:\n",
    "        return classes\n",
    "    train_raw = pd.read_csv(train_filename, header=None)\n",
    "    test_raw = pd.read_csv(test_filename, header=None)\n",
    "    longest_sent = max([len(sent) for sent in tf.contrib.learn.preprocessing.tokenizer(train_raw[2])])\n",
    "    print(\"The longest sentence in the training data has {} words.\".format(longest_sent))\n",
    "\n",
    "    return train_raw, test_raw, classes\n",
    "\n",
    "\n",
    "def extract_data(train_raw, test_raw):\n",
    "    \"\"\"Extract the document and class from each entry in the data.\"\"\"\n",
    "    x_train = train_raw[2]\n",
    "    y_train = train_raw[0] - 1  # Start enumeration at 0 instead of 1\n",
    "    x_test = test_raw[2]\n",
    "    y_test = test_raw[0] - 1\n",
    "    print('Size of training set: {0}'.format(len(x_train)))\n",
    "    print('Size of test set: {0}'.format(len(x_test)))\n",
    "    return x_train, np.array(y_train), x_test, np.array(y_test)\n",
    "\n",
    "def process_vocabulary(train_sentences, test_sentences,\n",
    "                       reuse=True, vocabulary_processor=None, extend=False, sequence_lengths=False):\n",
    "    \"\"\"Map words to integers, and then map sentences to integer sequences of length flags.max_doc_len, by truncating and\n",
    "       padding as needed. This leads to an integer matrix of data which is what TensorFlow can work with. The processor\n",
    "       is then saved to disk in a file determined by flags.\n",
    "\n",
    "    Args:\n",
    "       reuse: if True load the vocabulary_processor is loaded from disk if the file exists.\n",
    "       vocabulary_processor: if not None, and it was not loaded from disk, the passed vocabulary_processor is used.\n",
    "       extend: if True the vocabulary processor (loaded or passed) is extended.\n",
    "       sequence_lengths: Whether to list the length of each document.\n",
    "    \"\"\"\n",
    "    \n",
    "    vocabulary_processor_path = path.join(MODEL_DIRECTORY, VOCAB_PROCESSOR_FILENAME)\n",
    "    # If vocabulary_processor gets created/altered save it.\n",
    "    if reuse and path.isfile(vocabulary_processor_path):\n",
    "        vocabulary_processor = tf.contrib.learn.preprocessing.VocabularyProcessor.restore(vocabulary_processor_path)\n",
    "        save_vocab_processor = extend\n",
    "    elif vocabulary_processor is None:\n",
    "        vocabulary_processor = tf.contrib.learn.preprocessing.VocabularyProcessor(MAX_DOCUMENT_LENGTH)\n",
    "        vocabulary_processor.fit(train_sentences)\n",
    "        save_vocab_processor = True\n",
    "    elif extend:\n",
    "        vocabulary_processor.vocabulary_.freeze(False)\n",
    "        vocabulary_processor.fit(train_sentences)\n",
    "        save_vocab_processor = True\n",
    "    else:\n",
    "        save_vocab_processor = False\n",
    "\n",
    "    if train_sentences is not None:\n",
    "        train_bow = np.array(list(vocabulary_processor.transform(train_sentences)))\n",
    "    else:\n",
    "        train_bow = None\n",
    "    if test_sentences is not None:\n",
    "        test_bow = np.array(list(vocabulary_processor.transform(test_sentences)))\n",
    "    else:\n",
    "        test_bow = None\n",
    "    n_words = len(vocabulary_processor.vocabulary_)\n",
    "    print('Number of words in vocabulary: %d' % n_words)\n",
    "\n",
    "    if save_vocab_processor:\n",
    "        if not path.isdir(MODEL_DIRECTORY):\n",
    "            makedirs(MODEL_DIRECTORY)\n",
    "        vocabulary_processor.save(vocabulary_processor_path)\n",
    "\n",
    "    if sequence_lengths:\n",
    "        def calculate_lengths(arr):\n",
    "            return arr.shape[1] - (arr != 0)[:, ::-1].argmax(axis=1)\n",
    "        train_lengths = calculate_lengths(train_bow) if train_bow is not None else None\n",
    "        test_lengths = calculate_lengths(test_bow) if test_bow is not None else None\n",
    "    else:\n",
    "        train_lengths = test_lengths = None\n",
    "\n",
    "    return train_bow, test_bow, train_lengths, test_lengths, vocabulary_processor, n_words\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def preprocess_data(sequence_lengths=False):\n",
    "    '''\n",
    "    Load data, shuffle it, process the vocabulary and save to DATA_FILENAME, if not done already.\n",
    "    Returns processed data. NOTE: If the max_doc_len changes from a previous run,\n",
    "    then DATA_FILENAME should be deleted so that it can be properly recreated.\n",
    "    '''\n",
    "    preprocessed_path = path.join(MODEL_DIRECTORY, DATA_FILENAME)\n",
    "    if path.isfile(preprocessed_path):\n",
    "        with open(preprocessed_path, 'rb') as f:\n",
    "            train_raw, x_train, y_train, x_test, y_test, \\\n",
    "            train_lengths, test_lengths, classes = pickle.load(f)\n",
    "    else:\n",
    "        #Get the raw data, downloading if neccessary \n",
    "        train_raw, test_raw, classes = get_data(DATA_DIRECTORY)\n",
    "        \n",
    "        #Seeding is neccessary for reproducability\n",
    "        np.random.seed(TF_SEED)\n",
    "        \n",
    "        # Shuffle data to make the distribution of classes roughly stratified for each mini-batch.\n",
    "        # This is not necessary for full batch training, but is essential for mini-batch training.\n",
    "        train_raw = shuffle(train_raw)\n",
    "        test_raw = shuffle(test_raw)\n",
    "        train_sentences, y_train, test_sentences, y_test = extract_data(train_raw, test_raw)\n",
    "        # Encode the raw data as integer vectors.\n",
    "        x_train, x_test, train_lengths, test_lengths, _, _ = process_vocabulary(\n",
    "            train_sentences, test_sentences,\n",
    "            reuse=True, sequence_lengths=sequence_lengths)\n",
    "        # Save the processed data to avoid re-processing.\n",
    "        saved = False\n",
    "        with open(preprocessed_path, 'wb') as f:\n",
    "            try:\n",
    "                pickle.dump([train_raw, x_train, y_train, x_test, y_test,\n",
    "                             train_lengths, test_lengths, classes], f)\n",
    "                saved = True\n",
    "            except (OverflowError, MemoryError):\n",
    "                # Can happen if max-doc-len is large.\n",
    "                pass\n",
    "\n",
    "        if not saved:\n",
    "            remove(preprocessed_path)\n",
    "\n",
    "    return train_raw, x_train, y_train, x_test, y_test, train_lengths, test_lengths, classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Modelling: Training, evaluation and prediction. Also metadata for TensorBoard.\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "def input_fn(x, y=None, lengths=None, batch_size=None, num_epochs=None, shuffle=False):\n",
    "    \"\"\"Generic input function to be used as the input_fn arguments for Experiment or directly with Estimators.\"\"\"\n",
    "    if batch_size is None and x is not None:\n",
    "        batch_size = len(x)\n",
    "    x_dict = {WORDS_FEATURE: x}\n",
    "    if lengths is not None:\n",
    "        x_dict['LENGTHS_FEATURE'] = lengths\n",
    "    return tf.estimator.inputs.numpy_input_fn(\n",
    "        x_dict,\n",
    "        y,\n",
    "        batch_size=batch_size,\n",
    "        num_epochs=num_epochs,\n",
    "        shuffle=shuffle)\n",
    "\n",
    "\n",
    "def run_experiment(x_train, y_train, x_dev, y_dev, model_fn, schedule, output_dim, train_lengths=None, dev_lengths=None):\n",
    "    \"\"\"Create experiment object and run it.\"\"\"\n",
    "    hparams = tf.contrib.training.HParams(\n",
    "        n_words=MAX_VOCABULARY_SIZE,\n",
    "        n_epochs=NUM_EPOCHS,\n",
    "        seed=TF_SEED,\n",
    "        batch_size=BATCH_SIZE,\n",
    "        learning_rate=LEARNING_RATE,\n",
    "        output_dim=output_dim,\n",
    "        embed_dim=50)\n",
    "\n",
    "    is_training = schedule in ['train', 'train_and_evaluate']\n",
    "    run_config = tf.contrib.learn.RunConfig()\n",
    "    try:\n",
    "        checkpoint_steps = len(x_train) / CHECKPOINTS_PER_EPOCH / BATCH_SIZE if is_training else None\n",
    "        log_step_count_steps = 100  # default value\n",
    "    except TypeError:\n",
    "        # Happens if batch_size is None\n",
    "        checkpoint_steps = 1\n",
    "        log_step_count_steps = 1\n",
    "    run_config = run_config.replace(model_dir=MODEL_DIRECTORY,\n",
    "                                    save_checkpoints_steps=checkpoint_steps,\n",
    "                                    log_step_count_steps=log_step_count_steps,\n",
    "                                    tf_random_seed=hparams.seed)\n",
    "\n",
    "    def experiment_fn(run_config, hparams):\n",
    "        estimator = tf.estimator.Estimator(\n",
    "            model_fn=model_fn,\n",
    "            config=run_config,\n",
    "            params=hparams)\n",
    "        experiment = tf.contrib.learn.Experiment(\n",
    "            estimator=estimator,\n",
    "            train_input_fn=input_fn(x_train, y_train, train_lengths,\n",
    "                                    batch_size=hparams.batch_size,\n",
    "                                    num_epochs=hparams.n_epochs,\n",
    "                                    shuffle=True),\n",
    "            eval_input_fn=input_fn(x_dev, y_dev, dev_lengths,\n",
    "                                   num_epochs=1),\n",
    "            eval_delay_secs=0)\n",
    "        return experiment\n",
    "\n",
    "    if is_training:\n",
    "        print('Training model for {} epochs...'.format(hparams.n_epochs))\n",
    "    tf.contrib.learn.learn_runner.run(\n",
    "        experiment_fn=experiment_fn,\n",
    "        run_config=run_config,\n",
    "        schedule=schedule,  # What to run, e.g. \"train_and_evaluate\", \"evaluate\", ...\n",
    "        hparams=hparams)  # hyperparameters\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def estimator_spec_for_softmax_classification(logits, labels, mode, params):\n",
    "    \"\"\"Returns EstimatorSpec instance for softmax classification.\"\"\"\n",
    "    predicted_class = tf.argmax(logits, 1)\n",
    "    if mode == tf.estimator.ModeKeys.PREDICT:\n",
    "        return tf.estimator.EstimatorSpec(\n",
    "            mode=mode,\n",
    "            predictions={\n",
    "                'class': predicted_class,\n",
    "                'prob': tf.nn.softmax(logits)\n",
    "            })\n",
    "\n",
    "    loss = tf.losses.sparse_softmax_cross_entropy(labels=labels, logits=logits)\n",
    "    if mode == tf.estimator.ModeKeys.TRAIN:\n",
    "        with tf.name_scope('OptimizeLoss'):\n",
    "            optimizer = tf.train.AdamOptimizer(learning_rate=params.learning_rate)\n",
    "            train_op = optimizer.minimize(loss, global_step=tf.train.get_global_step())\n",
    "        return tf.estimator.EstimatorSpec(mode=mode, loss=loss, train_op=train_op)\n",
    "\n",
    "    # mode == EVAL\n",
    "    eval_metric_ops = {\n",
    "        'accuracy': tf.metrics.accuracy(labels=labels, predictions=predicted_class)\n",
    "    }\n",
    "    return tf.estimator.EstimatorSpec(mode=mode, loss=loss, eval_metric_ops=eval_metric_ops)\n",
    "\n",
    "def bag_of_words_MLP_model(features, labels, mode, params):\n",
    "    \"\"\"MLP architecture\"\"\"\n",
    "    with tf.variable_scope('MLP'):\n",
    "        bow_column = tf.feature_column.categorical_column_with_identity(\n",
    "            WORDS_FEATURE, num_buckets=params.n_words)\n",
    "        bow_embedding_column = tf.feature_column.embedding_column(\n",
    "            bow_column, dimension=params.embed_dim)\n",
    "        bow = tf.feature_column.input_layer(\n",
    "            features,\n",
    "            feature_columns=[bow_embedding_column])\n",
    "        bow_activated = tf.nn.relu(bow)\n",
    "        logits = tf.layers.dense(bow_activated, params.output_dim, activation=None)\n",
    "\n",
    "    return estimator_spec_for_softmax_classification(logits, labels, mode, params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The longest sentence in the training data has 1472 words.\n",
      "Size of training set: 560000\n",
      "Size of test set: 70000\n",
      "Number of words in vocabulary: 822383\n",
      "Training model for 2 epochs...\n",
      "WARNING:tensorflow:RunConfig.uid (from tensorflow.contrib.learn.python.learn.estimators.run_config) is experimental and may change or be removed at any time, and without warning.\n",
      "INFO:tensorflow:Using config: {'_num_ps_replicas': 0, '_keep_checkpoint_max': 5, '_task_type': None, '_save_checkpoints_secs': None, '_keep_checkpoint_every_n_hours': 10000, '_tf_config': gpu_options {\n",
      "  per_process_gpu_memory_fraction: 1\n",
      "}\n",
      ", '_environment': 'local', '_master': '', '_model_dir': 'perceptron_mlp_model', '_num_worker_replicas': 0, '_save_checkpoints_steps': 3500.0, '_is_chief': True, '_evaluation_master': '', '_log_step_count_steps': 100, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x0000023529E17470>, '_save_summary_steps': 100, '_task_id': 0, '_tf_random_seed': 4242, '_session_config': None}\n",
      "WARNING:tensorflow:RunConfig.uid (from tensorflow.contrib.learn.python.learn.estimators.run_config) is experimental and may change or be removed at any time, and without warning.\n",
      "WARNING:tensorflow:From C:\\Users\\I349732\\AppData\\Local\\Continuum\\anaconda3\\envs\\tfdeeplearning\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\monitors.py:269: BaseMonitor.__init__ (from tensorflow.contrib.learn.python.learn.monitors) is deprecated and will be removed after 2016-12-05.\n",
      "Instructions for updating:\n",
      "Monitors are deprecated. Please use tf.train.SessionRunHook.\n",
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Saving checkpoints for 1 into perceptron_mlp_model\\model.ckpt.\n",
      "INFO:tensorflow:Starting evaluation at 2019-03-27-10:13:12\n",
      "INFO:tensorflow:Restoring parameters from perceptron_mlp_model\\model.ckpt-1\n",
      "INFO:tensorflow:Evaluation [1/100]\n",
      "INFO:tensorflow:Finished evaluation at 2019-03-27-10:13:13\n",
      "INFO:tensorflow:Saving dict for global step 1: accuracy = 0.121471, global_step = 1, loss = 2.63206\n",
      "INFO:tensorflow:Validation (step 1): loss = 2.63206, accuracy = 0.121471, global_step = 1\n",
      "INFO:tensorflow:loss = 2.63787, step = 1\n",
      "INFO:tensorflow:global_step/sec: 2.32331\n",
      "INFO:tensorflow:loss = 0.417057, step = 101 (40.567 sec)\n",
      "INFO:tensorflow:global_step/sec: 2.35263\n",
      "INFO:tensorflow:loss = 0.571045, step = 201 (42.506 sec)\n",
      "INFO:tensorflow:global_step/sec: 2.43259\n",
      "INFO:tensorflow:loss = 0.455903, step = 301 (41.109 sec)\n",
      "INFO:tensorflow:global_step/sec: 2.28276\n",
      "INFO:tensorflow:loss = 0.329837, step = 401 (43.806 sec)\n",
      "INFO:tensorflow:global_step/sec: 2.32345\n",
      "INFO:tensorflow:loss = 0.286926, step = 501 (43.038 sec)\n",
      "INFO:tensorflow:global_step/sec: 2.33236\n",
      "INFO:tensorflow:loss = 0.1056, step = 601 (42.884 sec)\n",
      "INFO:tensorflow:global_step/sec: 2.37655\n",
      "INFO:tensorflow:loss = 0.561133, step = 701 (42.072 sec)\n",
      "INFO:tensorflow:global_step/sec: 2.21905\n",
      "INFO:tensorflow:loss = 0.531407, step = 801 (45.063 sec)\n",
      "INFO:tensorflow:global_step/sec: 2.08324\n",
      "INFO:tensorflow:loss = 0.427364, step = 901 (48.008 sec)\n",
      "INFO:tensorflow:global_step/sec: 2.20084\n",
      "INFO:tensorflow:loss = 0.56786, step = 1001 (45.430 sec)\n",
      "INFO:tensorflow:global_step/sec: 2.21371\n",
      "INFO:tensorflow:loss = 0.491771, step = 1101 (45.173 sec)\n",
      "INFO:tensorflow:global_step/sec: 2.21521\n",
      "INFO:tensorflow:loss = 0.515553, step = 1201 (45.204 sec)\n",
      "INFO:tensorflow:global_step/sec: 2.08126\n",
      "INFO:tensorflow:loss = 0.512323, step = 1301 (47.991 sec)\n",
      "INFO:tensorflow:global_step/sec: 1.99469\n",
      "INFO:tensorflow:loss = 0.437237, step = 1401 (50.133 sec)\n",
      "INFO:tensorflow:global_step/sec: 1.94534\n",
      "INFO:tensorflow:loss = 0.308892, step = 1501 (51.411 sec)\n",
      "INFO:tensorflow:global_step/sec: 2.06673\n",
      "INFO:tensorflow:loss = 0.132426, step = 1601 (48.380 sec)\n",
      "INFO:tensorflow:global_step/sec: 2.09346\n",
      "INFO:tensorflow:loss = 0.258982, step = 1701 (47.765 sec)\n",
      "INFO:tensorflow:global_step/sec: 2.11089\n",
      "INFO:tensorflow:loss = 0.630263, step = 1801 (47.380 sec)\n",
      "INFO:tensorflow:global_step/sec: 2.35811\n",
      "INFO:tensorflow:loss = 0.352567, step = 1901 (42.398 sec)\n",
      "INFO:tensorflow:global_step/sec: 2.23792\n",
      "INFO:tensorflow:loss = 0.349202, step = 2001 (44.684 sec)\n",
      "INFO:tensorflow:global_step/sec: 2.3147\n",
      "INFO:tensorflow:loss = 0.390197, step = 2101 (43.210 sec)\n",
      "INFO:tensorflow:global_step/sec: 2.31818\n",
      "INFO:tensorflow:loss = 0.10153, step = 2201 (43.122 sec)\n",
      "INFO:tensorflow:global_step/sec: 2.26304\n",
      "INFO:tensorflow:loss = 0.273517, step = 2301 (44.194 sec)\n",
      "INFO:tensorflow:global_step/sec: 2.26548\n",
      "INFO:tensorflow:loss = 0.128086, step = 2401 (44.152 sec)\n",
      "INFO:tensorflow:global_step/sec: 2.3269\n",
      "INFO:tensorflow:loss = 0.296181, step = 2501 (42.974 sec)\n",
      "INFO:tensorflow:global_step/sec: 2.47841\n",
      "INFO:tensorflow:loss = 0.459347, step = 2601 (40.333 sec)\n",
      "INFO:tensorflow:global_step/sec: 2.40658\n",
      "INFO:tensorflow:loss = 0.802449, step = 2701 (41.563 sec)\n",
      "INFO:tensorflow:global_step/sec: 2.23903\n",
      "INFO:tensorflow:loss = 0.767659, step = 2801 (44.658 sec)\n",
      "INFO:tensorflow:global_step/sec: 2.43548\n",
      "INFO:tensorflow:loss = 0.409808, step = 2901 (41.061 sec)\n",
      "INFO:tensorflow:global_step/sec: 2.30413\n",
      "INFO:tensorflow:loss = 0.271752, step = 3001 (43.403 sec)\n",
      "INFO:tensorflow:global_step/sec: 2.24804\n",
      "INFO:tensorflow:loss = 0.476545, step = 3101 (44.473 sec)\n",
      "INFO:tensorflow:global_step/sec: 2.48263\n",
      "INFO:tensorflow:loss = 0.152227, step = 3201 (40.280 sec)\n",
      "INFO:tensorflow:global_step/sec: 2.35569\n",
      "INFO:tensorflow:loss = 0.286823, step = 3301 (42.465 sec)\n",
      "INFO:tensorflow:global_step/sec: 2.42916\n",
      "INFO:tensorflow:loss = 0.422485, step = 3401 (41.157 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 3501 into perceptron_mlp_model\\model.ckpt.\n",
      "INFO:tensorflow:global_step/sec: 2.23762\n",
      "INFO:tensorflow:Starting evaluation at 2019-03-27-10:39:01\n",
      "INFO:tensorflow:Restoring parameters from perceptron_mlp_model\\model.ckpt-3501\n",
      "INFO:tensorflow:Evaluation [1/100]\n",
      "INFO:tensorflow:Finished evaluation at 2019-03-27-10:39:02\n",
      "INFO:tensorflow:Saving dict for global step 3501: accuracy = 0.881543, global_step = 3501, loss = 0.342097\n",
      "INFO:tensorflow:Validation (step 3501): loss = 0.342097, accuracy = 0.881543, global_step = 3501\n",
      "INFO:tensorflow:loss = 0.37141, step = 3501 (46.383 sec)\n",
      "INFO:tensorflow:global_step/sec: 2.30365\n",
      "INFO:tensorflow:loss = 0.0564464, step = 3601 (41.726 sec)\n",
      "INFO:tensorflow:global_step/sec: 2.23751\n",
      "INFO:tensorflow:loss = 0.428056, step = 3701 (44.691 sec)\n",
      "INFO:tensorflow:global_step/sec: 2.1633\n",
      "INFO:tensorflow:loss = 0.214863, step = 3801 (46.213 sec)\n",
      "INFO:tensorflow:global_step/sec: 2.49277\n",
      "INFO:tensorflow:loss = 0.1422, step = 3901 (40.132 sec)\n",
      "INFO:tensorflow:global_step/sec: 2.57854\n",
      "INFO:tensorflow:loss = 0.333732, step = 4001 (38.782 sec)\n",
      "INFO:tensorflow:global_step/sec: 2.67785\n",
      "INFO:tensorflow:loss = 0.356383, step = 4101 (37.328 sec)\n",
      "INFO:tensorflow:global_step/sec: 2.63892\n",
      "INFO:tensorflow:loss = 0.172843, step = 4201 (37.910 sec)\n",
      "INFO:tensorflow:global_step/sec: 2.65795\n",
      "INFO:tensorflow:loss = 0.413736, step = 4301 (37.607 sec)\n",
      "INFO:tensorflow:global_step/sec: 2.6702\n",
      "INFO:tensorflow:loss = 0.233879, step = 4401 (37.466 sec)\n",
      "INFO:tensorflow:global_step/sec: 2.64391\n",
      "INFO:tensorflow:loss = 0.28383, step = 4501 (37.807 sec)\n",
      "INFO:tensorflow:global_step/sec: 2.64432\n",
      "INFO:tensorflow:loss = 0.0972632, step = 4601 (37.817 sec)\n",
      "INFO:tensorflow:global_step/sec: 2.65222\n",
      "INFO:tensorflow:loss = 0.513959, step = 4701 (37.704 sec)\n",
      "INFO:tensorflow:global_step/sec: 2.41794\n",
      "INFO:tensorflow:loss = 0.253388, step = 4801 (41.358 sec)\n",
      "INFO:tensorflow:global_step/sec: 2.67272\n",
      "INFO:tensorflow:loss = 0.821537, step = 4901 (37.415 sec)\n",
      "INFO:tensorflow:global_step/sec: 2.67283\n",
      "INFO:tensorflow:loss = 0.332631, step = 5001 (37.414 sec)\n",
      "INFO:tensorflow:global_step/sec: 2.55395\n",
      "INFO:tensorflow:loss = 0.167866, step = 5101 (39.171 sec)\n",
      "INFO:tensorflow:global_step/sec: 2.68899\n",
      "INFO:tensorflow:loss = 0.54156, step = 5201 (37.189 sec)\n",
      "INFO:tensorflow:global_step/sec: 2.62913\n",
      "INFO:tensorflow:loss = 0.345222, step = 5301 (38.020 sec)\n",
      "INFO:tensorflow:global_step/sec: 2.66466\n",
      "INFO:tensorflow:loss = 0.505321, step = 5401 (37.528 sec)\n",
      "INFO:tensorflow:global_step/sec: 2.66949\n",
      "INFO:tensorflow:loss = 0.301288, step = 5501 (37.476 sec)\n",
      "INFO:tensorflow:global_step/sec: 2.62527\n",
      "INFO:tensorflow:loss = 0.505191, step = 5601 (38.076 sec)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:global_step/sec: 2.68241\n",
      "INFO:tensorflow:loss = 0.271131, step = 5701 (37.296 sec)\n",
      "INFO:tensorflow:global_step/sec: 2.6741\n",
      "INFO:tensorflow:loss = 0.193376, step = 5801 (37.396 sec)\n",
      "INFO:tensorflow:global_step/sec: 2.67055\n",
      "INFO:tensorflow:loss = 0.13423, step = 5901 (37.445 sec)\n",
      "INFO:tensorflow:global_step/sec: 2.67315\n",
      "INFO:tensorflow:loss = 0.301037, step = 6001 (37.393 sec)\n",
      "INFO:tensorflow:global_step/sec: 2.67\n",
      "INFO:tensorflow:loss = 0.404046, step = 6101 (37.453 sec)\n",
      "INFO:tensorflow:global_step/sec: 2.66655\n",
      "INFO:tensorflow:loss = 0.201012, step = 6201 (37.502 sec)\n",
      "INFO:tensorflow:global_step/sec: 2.66367\n",
      "INFO:tensorflow:loss = 0.423598, step = 6301 (37.542 sec)\n",
      "INFO:tensorflow:global_step/sec: 2.63717\n",
      "INFO:tensorflow:loss = 0.270981, step = 6401 (37.935 sec)\n",
      "INFO:tensorflow:global_step/sec: 2.67555\n",
      "INFO:tensorflow:loss = 0.397365, step = 6501 (37.376 sec)\n",
      "INFO:tensorflow:global_step/sec: 2.6722\n",
      "INFO:tensorflow:loss = 0.298081, step = 6601 (37.407 sec)\n",
      "INFO:tensorflow:global_step/sec: 2.67666\n",
      "INFO:tensorflow:loss = 0.194216, step = 6701 (37.376 sec)\n",
      "INFO:tensorflow:global_step/sec: 2.64984\n",
      "INFO:tensorflow:loss = 0.286017, step = 6801 (37.738 sec)\n",
      "INFO:tensorflow:global_step/sec: 2.65517\n",
      "INFO:tensorflow:loss = 0.217076, step = 6901 (37.662 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 7001 into perceptron_mlp_model\\model.ckpt.\n",
      "INFO:tensorflow:global_step/sec: 2.39955\n",
      "INFO:tensorflow:Starting evaluation at 2019-03-27-11:01:30\n",
      "INFO:tensorflow:Restoring parameters from perceptron_mlp_model\\model.ckpt-7001\n",
      "INFO:tensorflow:Evaluation [1/100]\n",
      "INFO:tensorflow:Finished evaluation at 2019-03-27-11:01:32\n",
      "INFO:tensorflow:Saving dict for global step 7001: accuracy = 0.893857, global_step = 7001, loss = 0.307141\n",
      "INFO:tensorflow:Validation (step 7001): loss = 0.307141, accuracy = 0.893857, global_step = 7001\n",
      "INFO:tensorflow:loss = 0.419146, step = 7001 (43.096 sec)\n",
      "INFO:tensorflow:global_step/sec: 2.52975\n",
      "INFO:tensorflow:loss = 0.339917, step = 7101 (38.109 sec)\n",
      "INFO:tensorflow:global_step/sec: 2.66973\n",
      "INFO:tensorflow:loss = 0.3695, step = 7201 (37.441 sec)\n",
      "INFO:tensorflow:global_step/sec: 2.68415\n",
      "INFO:tensorflow:loss = 0.0722242, step = 7301 (37.271 sec)\n",
      "INFO:tensorflow:global_step/sec: 2.68891\n",
      "INFO:tensorflow:loss = 0.259609, step = 7401 (37.174 sec)\n",
      "INFO:tensorflow:global_step/sec: 2.71732\n",
      "INFO:tensorflow:loss = 0.123758, step = 7501 (36.801 sec)\n",
      "INFO:tensorflow:global_step/sec: 2.68788\n",
      "INFO:tensorflow:loss = 0.395721, step = 7601 (37.204 sec)\n",
      "INFO:tensorflow:global_step/sec: 2.68349\n",
      "INFO:tensorflow:loss = 0.172985, step = 7701 (37.265 sec)\n",
      "INFO:tensorflow:global_step/sec: 2.69438\n",
      "INFO:tensorflow:loss = 0.134893, step = 7801 (37.114 sec)\n",
      "INFO:tensorflow:global_step/sec: 2.65017\n",
      "INFO:tensorflow:loss = 0.223776, step = 7901 (37.749 sec)\n",
      "INFO:tensorflow:global_step/sec: 2.68662\n",
      "INFO:tensorflow:loss = 0.460714, step = 8001 (37.206 sec)\n",
      "INFO:tensorflow:global_step/sec: 2.67788\n",
      "INFO:tensorflow:loss = 0.450892, step = 8101 (37.359 sec)\n",
      "INFO:tensorflow:global_step/sec: 2.65093\n",
      "INFO:tensorflow:loss = 0.32707, step = 8201 (37.723 sec)\n",
      "INFO:tensorflow:global_step/sec: 2.69391\n",
      "INFO:tensorflow:loss = 0.177859, step = 8301 (37.121 sec)\n",
      "INFO:tensorflow:global_step/sec: 2.68113\n",
      "INFO:tensorflow:loss = 0.377661, step = 8401 (37.282 sec)\n",
      "INFO:tensorflow:global_step/sec: 2.65798\n",
      "INFO:tensorflow:loss = 0.296798, step = 8501 (37.623 sec)\n",
      "INFO:tensorflow:global_step/sec: 2.66425\n",
      "INFO:tensorflow:loss = 0.375595, step = 8601 (37.534 sec)\n",
      "INFO:tensorflow:global_step/sec: 2.67902\n",
      "INFO:tensorflow:loss = 0.378105, step = 8701 (37.343 sec)\n",
      "INFO:tensorflow:global_step/sec: 2.67948\n",
      "INFO:tensorflow:loss = 0.192058, step = 8801 (37.305 sec)\n",
      "INFO:tensorflow:global_step/sec: 2.67327\n",
      "INFO:tensorflow:loss = 0.267191, step = 8901 (37.423 sec)\n",
      "INFO:tensorflow:global_step/sec: 2.63106\n",
      "INFO:tensorflow:loss = 0.445712, step = 9001 (37.992 sec)\n",
      "INFO:tensorflow:global_step/sec: 2.55707\n",
      "INFO:tensorflow:loss = 0.207344, step = 9101 (39.123 sec)\n",
      "INFO:tensorflow:global_step/sec: 2.58473\n",
      "INFO:tensorflow:loss = 0.138283, step = 9201 (38.673 sec)\n",
      "INFO:tensorflow:global_step/sec: 2.6526\n",
      "INFO:tensorflow:loss = 0.312643, step = 9301 (37.714 sec)\n",
      "INFO:tensorflow:global_step/sec: 2.58399\n",
      "INFO:tensorflow:loss = 0.328376, step = 9401 (38.684 sec)\n",
      "INFO:tensorflow:global_step/sec: 2.62898\n",
      "INFO:tensorflow:loss = 0.287429, step = 9501 (38.053 sec)\n",
      "INFO:tensorflow:global_step/sec: 2.59417\n",
      "INFO:tensorflow:loss = 0.432093, step = 9601 (38.548 sec)\n",
      "INFO:tensorflow:global_step/sec: 2.58525\n",
      "INFO:tensorflow:loss = 0.14173, step = 9701 (38.681 sec)\n",
      "INFO:tensorflow:global_step/sec: 2.55914\n",
      "INFO:tensorflow:loss = 0.0220914, step = 9801 (39.076 sec)\n",
      "INFO:tensorflow:global_step/sec: 2.53297\n",
      "INFO:tensorflow:loss = 0.261612, step = 9901 (39.464 sec)\n",
      "INFO:tensorflow:global_step/sec: 2.57963\n",
      "INFO:tensorflow:loss = 1.04369, step = 10001 (38.765 sec)\n",
      "INFO:tensorflow:global_step/sec: 2.59355\n",
      "INFO:tensorflow:loss = 0.788718, step = 10101 (38.573 sec)\n",
      "INFO:tensorflow:global_step/sec: 2.59658\n",
      "INFO:tensorflow:loss = 0.415639, step = 10201 (38.497 sec)\n",
      "INFO:tensorflow:global_step/sec: 2.57324\n",
      "INFO:tensorflow:loss = 0.245055, step = 10301 (38.877 sec)\n",
      "INFO:tensorflow:global_step/sec: 2.59921\n",
      "INFO:tensorflow:loss = 0.534789, step = 10401 (38.458 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 10501 into perceptron_mlp_model\\model.ckpt.\n",
      "INFO:tensorflow:global_step/sec: 2.41217\n",
      "INFO:tensorflow:Starting evaluation at 2019-03-27-11:23:43\n",
      "INFO:tensorflow:Restoring parameters from perceptron_mlp_model\\model.ckpt-10501\n",
      "INFO:tensorflow:Evaluation [1/100]\n",
      "INFO:tensorflow:Finished evaluation at 2019-03-27-11:23:44\n",
      "INFO:tensorflow:Saving dict for global step 10501: accuracy = 0.894743, global_step = 10501, loss = 0.315059\n",
      "INFO:tensorflow:Validation (step 10501): loss = 0.315059, accuracy = 0.894743, global_step = 10501\n",
      "INFO:tensorflow:loss = 0.367657, step = 10501 (43.268 sec)\n",
      "INFO:tensorflow:global_step/sec: 2.45307\n",
      "INFO:tensorflow:loss = 0.279577, step = 10601 (38.969 sec)\n",
      "INFO:tensorflow:global_step/sec: 2.29484\n",
      "INFO:tensorflow:loss = 0.206493, step = 10701 (43.576 sec)\n",
      "INFO:tensorflow:global_step/sec: 2.46195\n",
      "INFO:tensorflow:loss = 0.169967, step = 10801 (40.603 sec)\n",
      "INFO:tensorflow:global_step/sec: 2.35236\n",
      "INFO:tensorflow:loss = 0.217121, step = 10901 (42.525 sec)\n",
      "INFO:tensorflow:global_step/sec: 2.33496\n",
      "INFO:tensorflow:loss = 0.214284, step = 11001 (42.820 sec)\n"
     ]
    }
   ],
   "source": [
    "tic()\n",
    "train_raw, x_train, y_train, x_test, y_test, _, _, classes = preprocess_data()\n",
    "output_dim = len(classes)\n",
    "run_experiment(x_train, y_train, x_test, y_test,\n",
    "                   bag_of_words_MLP_model, 'train_and_evaluate', output_dim)\n",
    "toc()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
