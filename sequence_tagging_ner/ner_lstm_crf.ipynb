{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import functools\n",
    "import json\n",
    "import logging\n",
    "from pathlib import Path\n",
    "import sys\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tf_metrics import precision, recall, f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "DATADIR = './data'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Logging\n",
    "Path('results').mkdir(exist_ok=True)\n",
    "tf.logging.set_verbosity(logging.INFO)\n",
    "handlers = [\n",
    "    logging.FileHandler('results/main.log'),\n",
    "    logging.StreamHandler(sys.stdout)\n",
    "]\n",
    "logging.getLogger('tensorflow').handlers = handlers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def parse_fn(line_words, line_tags):\n",
    "    '''\n",
    "    Parses and converts words to bytes for a sentence.\n",
    "    Asserts that tag sequence length is equal to word sequence length\n",
    "    '''\n",
    "    words = [word.encode() for word in line_words.strip().split()]\n",
    "    tags = [tag.encode() for tag in line_tags.strip().split()]\n",
    "    assert(len(words) == len(tags))\n",
    "    return (words, len(words)), tags\n",
    "\n",
    "def generator_fn(words, tags):\n",
    "    '''\n",
    "    Opens sentences and tags file, reads, parses it and streams it\n",
    "    '''\n",
    "    with Path(words).open('r') as f_words, Path(tags).open('r') as f_tags:\n",
    "        for line_words, line_tags in zip(f_words, f_tags):\n",
    "            yield parse_fn(line_words, line_tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def input_fn(words, tags, params=None, shuffle_and_repeat=False):\n",
    "    '''\n",
    "    Input function for tf estimator. Responsible for dataset preprocessing\n",
    "    '''\n",
    "    params = params if params is not None else {}\n",
    "    shapes = (([None], ()), [None])\n",
    "    types = ((tf.string, tf.int32), tf.string)\n",
    "    defaults = (('<pad>', 0), 'O')\n",
    "\n",
    "    dataset = tf.data.Dataset.from_generator(\n",
    "         functools.partial(generator_fn, words, tags),\n",
    "         output_shapes=shapes, output_types=types)\n",
    "\n",
    "    if shuffle_and_repeat:\n",
    "         dataset = dataset.shuffle(params['buffer']).repeat(params['epochs'])\n",
    "\n",
    "    dataset = (dataset\n",
    "                .padded_batch(params.get('batch_size', 20), shapes, defaults)\n",
    "                .prefetch(1))\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def model_fn(features, labels, mode, params):\n",
    "    # For serving, features are a bit different\n",
    "    if isinstance(features, dict):\n",
    "        features = features['words'], features['nwords']\n",
    "\n",
    "    # Read vocabs and inputs\n",
    "    dropout = params['dropout']\n",
    "    words, nwords = features\n",
    "    training = (mode == tf.estimator.ModeKeys.TRAIN)\n",
    "    vocab_words = tf.contrib.lookup.index_table_from_file(\n",
    "        params['words'], num_oov_buckets=params['num_oov_buckets'])\n",
    "    with Path(params['tags']).open() as f:\n",
    "        indices = [idx for idx, tag in enumerate(f) if tag.strip() != 'O']\n",
    "        num_tags = len(indices) + 1\n",
    "\n",
    "    # Word Embeddings\n",
    "    word_ids = vocab_words.lookup(words)\n",
    "    glove = np.load(params['glove'])['embeddings']  # np.array\n",
    "    variable = np.vstack([glove, [[0.]*params['dim']]])\n",
    "    variable = tf.Variable(variable, dtype=tf.float32, trainable=False)\n",
    "    embeddings = tf.nn.embedding_lookup(variable, word_ids)\n",
    "    embeddings = tf.layers.dropout(embeddings, rate=dropout, training=training)\n",
    "\n",
    "    # LSTM\n",
    "    t = tf.transpose(embeddings, perm=[1, 0, 2])\n",
    "    lstm_cell_fw = tf.contrib.rnn.LSTMBlockFusedCell(params['lstm_size'])\n",
    "    lstm_cell_bw = tf.contrib.rnn.LSTMBlockFusedCell(params['lstm_size'])\n",
    "    lstm_cell_bw = tf.contrib.rnn.TimeReversedFusedRNN(lstm_cell_bw)\n",
    "    output_fw, _ = lstm_cell_fw(t, dtype=tf.float32, sequence_length=nwords)\n",
    "    output_bw, _ = lstm_cell_bw(t, dtype=tf.float32, sequence_length=nwords)\n",
    "    output = tf.concat([output_fw, output_bw], axis=-1)\n",
    "    output = tf.transpose(output, perm=[1, 0, 2])\n",
    "    output = tf.layers.dropout(output, rate=dropout, training=training)\n",
    "\n",
    "    # CRF\n",
    "    logits = tf.layers.dense(output, num_tags)\n",
    "    crf_params = tf.get_variable(\"crf\", [num_tags, num_tags], dtype=tf.float32)\n",
    "    pred_ids, _ = tf.contrib.crf.crf_decode(logits, crf_params, nwords)\n",
    "\n",
    "    if mode == tf.estimator.ModeKeys.PREDICT:\n",
    "        # Predictions\n",
    "        reverse_vocab_tags = tf.contrib.lookup.index_to_string_table_from_file(\n",
    "            params['tags'])\n",
    "        pred_strings = reverse_vocab_tags.lookup(tf.to_int64(pred_ids))\n",
    "        predictions = {\n",
    "            'pred_ids': pred_ids,\n",
    "            'tags': pred_strings\n",
    "        }\n",
    "        return tf.estimator.EstimatorSpec(mode, predictions=predictions)\n",
    "    else:\n",
    "        # Loss\n",
    "        vocab_tags = tf.contrib.lookup.index_table_from_file(params['tags'])\n",
    "        tags = vocab_tags.lookup(labels)\n",
    "        log_likelihood, _ = tf.contrib.crf.crf_log_likelihood(\n",
    "            logits, tags, nwords, crf_params)\n",
    "        loss = tf.reduce_mean(-log_likelihood)\n",
    "\n",
    "        # Metrics\n",
    "        weights = tf.sequence_mask(nwords)\n",
    "        metrics = {\n",
    "            'acc': tf.metrics.accuracy(tags, pred_ids, weights),\n",
    "            'precision': precision(tags, pred_ids, num_tags, indices, weights),\n",
    "            'recall': recall(tags, pred_ids, num_tags, indices, weights),\n",
    "            'f1': f1(tags, pred_ids, num_tags, indices, weights),\n",
    "        }\n",
    "        for metric_name, op in metrics.items():\n",
    "            tf.summary.scalar(metric_name, op[1])\n",
    "\n",
    "        if mode == tf.estimator.ModeKeys.EVAL:\n",
    "            return tf.estimator.EstimatorSpec(\n",
    "                mode, loss=loss, eval_metric_ops=metrics)\n",
    "\n",
    "        elif mode == tf.estimator.ModeKeys.TRAIN:\n",
    "            train_op = tf.train.AdamOptimizer().minimize(\n",
    "                loss, global_step=tf.train.get_or_create_global_step())\n",
    "            return tf.estimator.EstimatorSpec(\n",
    "                mode, loss=loss, train_op=train_op)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using config: {'_save_summary_steps': 100, '_is_chief': True, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x0000017D3C7B5400>, '_keep_checkpoint_every_n_hours': 10000, '_save_checkpoints_secs': 120, '_master': '', '_log_step_count_steps': 100, '_task_type': 'worker', '_keep_checkpoint_max': 5, '_save_checkpoints_steps': None, '_num_ps_replicas': 0, '_tf_random_seed': None, '_evaluation_master': '', '_session_config': None, '_model_dir': 'results/model', '_global_id_in_cluster': 0, '_task_id': 0, '_service': None, '_num_worker_replicas': 1}\n",
      "Using config: {'_save_summary_steps': 100, '_is_chief': True, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x0000017D3C8B17B8>, '_keep_checkpoint_every_n_hours': 10000, '_save_checkpoints_secs': 120, '_master': '', '_log_step_count_steps': 100, '_task_type': 'worker', '_keep_checkpoint_max': 5, '_save_checkpoints_steps': None, '_num_ps_replicas': 0, '_tf_random_seed': None, '_evaluation_master': '', '_session_config': None, '_model_dir': 'results/model', '_global_id_in_cluster': 0, '_task_id': 0, '_service': None, '_num_worker_replicas': 1}\n",
      "Running training and evaluation locally (non-distributed).\n",
      "Start train and evaluate loop. The evaluate will happen after 120 secs (eval_spec.throttle_secs) or training is finished.\n",
      "Calling model_fn.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\I349732\\AppData\\Local\\Continuum\\anaconda3\\envs\\tfdeeplearning\\lib\\site-packages\\tensorflow\\python\\ops\\gradients_impl.py:100: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done calling model_fn.\n",
      "Create CheckpointSaverHook.\n",
      "Graph was finalized.\n",
      "Running local_init_op.\n",
      "Done running local_init_op.\n",
      "Saving checkpoints for 1 into results/model\\model.ckpt.\n",
      "step = 1, loss = 52.9785\n",
      "global_step/sec: 2.51487\n",
      "step = 101, loss = 7.26186 (39.765 sec)\n",
      "global_step/sec: 2.58494\n",
      "step = 201, loss = 5.3284 (38.686 sec)\n",
      "Saving checkpoints for 240 into results/model\\model.ckpt.\n",
      "Loss for final step: 9.12751.\n",
      "Calling model_fn.\n",
      "Done calling model_fn.\n",
      "Starting evaluation at 2019-04-01-08:20:36\n",
      "Graph was finalized.\n",
      "Restoring parameters from results/model\\model.ckpt-240\n",
      "Running local_init_op.\n",
      "Done running local_init_op.\n",
      "Evaluation [10/100]\n",
      "Evaluation [20/100]\n",
      "Evaluation [30/100]\n",
      "Evaluation [40/100]\n",
      "Evaluation [50/100]\n",
      "Evaluation [60/100]\n",
      "Evaluation [70/100]\n",
      "Evaluation [80/100]\n",
      "Evaluation [90/100]\n",
      "Finished evaluation at 2019-04-01-08:20:52\n",
      "Saving dict for global step 240: acc = 0.947817, f1 = 0.800551, global_step = 240, loss = 4.38134, precision = 0.867535, recall = 0.743169\n",
      "Calling model_fn.\n",
      "Done calling model_fn.\n",
      "Create CheckpointSaverHook.\n",
      "Graph was finalized.\n",
      "Restoring parameters from results/model\\model.ckpt-240\n",
      "Running local_init_op.\n",
      "Done running local_init_op.\n",
      "Saving checkpoints for 241 into results/model\\model.ckpt.\n",
      "step = 241, loss = 6.16917\n",
      "global_step/sec: 2.19685\n",
      "step = 341, loss = 5.5664 (45.522 sec)\n",
      "global_step/sec: 2.57959\n",
      "step = 441, loss = 8.6558 (38.774 sec)\n",
      "Saving checkpoints for 471 into results/model\\model.ckpt.\n",
      "Loss for final step: 3.5366.\n",
      "Calling model_fn.\n",
      "Done calling model_fn.\n",
      "Starting evaluation at 2019-04-01-08:23:06\n",
      "Graph was finalized.\n",
      "Restoring parameters from results/model\\model.ckpt-471\n",
      "Running local_init_op.\n",
      "Done running local_init_op.\n",
      "Evaluation [10/100]\n",
      "Evaluation [20/100]\n",
      "Evaluation [30/100]\n",
      "Evaluation [40/100]\n",
      "Evaluation [50/100]\n",
      "Evaluation [60/100]\n",
      "Evaluation [70/100]\n",
      "Evaluation [80/100]\n",
      "Evaluation [90/100]\n",
      "Finished evaluation at 2019-04-01-08:23:22\n",
      "Saving dict for global step 471: acc = 0.963723, f1 = 0.859051, global_step = 471, loss = 3.17574, precision = 0.898768, recall = 0.822695\n",
      "Calling model_fn.\n",
      "Done calling model_fn.\n",
      "Create CheckpointSaverHook.\n",
      "Graph was finalized.\n",
      "Restoring parameters from results/model\\model.ckpt-471\n",
      "Running local_init_op.\n",
      "Done running local_init_op.\n",
      "Saving checkpoints for 472 into results/model\\model.ckpt.\n",
      "step = 472, loss = 9.60308\n",
      "global_step/sec: 2.66162\n",
      "step = 572, loss = 3.2669 (37.573 sec)\n",
      "global_step/sec: 2.33858\n",
      "step = 672, loss = 3.76126 (42.770 sec)\n",
      "Saving checkpoints for 712 into results/model\\model.ckpt.\n",
      "Loss for final step: 3.06678.\n",
      "Calling model_fn.\n",
      "Done calling model_fn.\n",
      "Starting evaluation at 2019-04-01-08:25:33\n",
      "Graph was finalized.\n",
      "Restoring parameters from results/model\\model.ckpt-712\n",
      "Running local_init_op.\n",
      "Done running local_init_op.\n",
      "Evaluation [10/100]\n",
      "Evaluation [20/100]\n",
      "Evaluation [30/100]\n",
      "Evaluation [40/100]\n",
      "Evaluation [50/100]\n",
      "Evaluation [60/100]\n",
      "Evaluation [70/100]\n",
      "Evaluation [80/100]\n",
      "Evaluation [90/100]\n",
      "Finished evaluation at 2019-04-01-08:25:51\n",
      "Saving dict for global step 712: acc = 0.968972, f1 = 0.875807, global_step = 712, loss = 2.59745, precision = 0.910003, recall = 0.844088\n",
      "Calling model_fn.\n",
      "Done calling model_fn.\n",
      "Create CheckpointSaverHook.\n",
      "Graph was finalized.\n",
      "Restoring parameters from results/model\\model.ckpt-712\n",
      "Running local_init_op.\n",
      "Done running local_init_op.\n",
      "Saving checkpoints for 713 into results/model\\model.ckpt.\n",
      "step = 713, loss = 5.38727\n",
      "global_step/sec: 2.17175\n",
      "step = 813, loss = 2.47588 (46.048 sec)\n"
     ]
    }
   ],
   "source": [
    "params = {\n",
    "    'dim': 300,\n",
    "    'dropout': 0.5,\n",
    "    'num_oov_buckets': 1,\n",
    "    'epochs': 25,\n",
    "    'batch_size': 20,\n",
    "    'buffer': 15000,\n",
    "    'lstm_size': 100,\n",
    "    'words': str(Path(DATADIR, 'vocab.words.txt')),\n",
    "    'chars': str(Path(DATADIR, 'vocab.chars.txt')),\n",
    "    'tags': str(Path(DATADIR, 'vocab.tags.txt')),\n",
    "    'glove': str(Path(DATADIR, 'glove.npz'))\n",
    "}\n",
    "with Path('results/params.json').open('w') as f:\n",
    "        json.dump(params, f, indent=4, sort_keys=True)\n",
    "def fwords(name):\n",
    "        return str(Path(DATADIR, '{}.words.txt'.format(name)))\n",
    "def ftags(name):\n",
    "        return str(Path(DATADIR, '{}.tags.txt'.format(name)))\n",
    "    \n",
    "cfg = tf.estimator.RunConfig(save_checkpoints_secs=120)\n",
    "estimator = tf.estimator.Estimator(model_fn, 'results/model', cfg, params)\n",
    "# Estimator, train and evaluate\n",
    "train_inpf = functools.partial(input_fn, fwords('train'), ftags('train'),\n",
    "                                   params, shuffle_and_repeat=True)\n",
    "eval_inpf = functools.partial(input_fn, fwords('testa'), ftags('testa'))\n",
    "\n",
    "cfg = tf.estimator.RunConfig(save_checkpoints_secs=120)\n",
    "estimator = tf.estimator.Estimator(model_fn, 'results/model', cfg, params)\n",
    "\n",
    "train_spec = tf.estimator.TrainSpec(input_fn=train_inpf)\n",
    "eval_spec = tf.estimator.EvalSpec(input_fn=eval_inpf, throttle_secs=120)\n",
    "tf.estimator.train_and_evaluate(estimator, train_spec, eval_spec)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
